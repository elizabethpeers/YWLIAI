{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D, GlobalAvgPool2D, BatchNormalization, add, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "#from __future__ import print_function\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from io import open\n",
    "from zipfile import ZipFile\n",
    "from keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'interference-image.jpg',\n",
       " 'Rex the robot-Copy1.ipynb',\n",
       " 'Rex the robot.ipynb',\n",
       " 'Rex the robot.py',\n",
       " 'rex-images',\n",
       " 'rex-the-robot-dataset',\n",
       " 'rex_models',\n",
       " 'rex_model_class.json',\n",
       " 'rex_weight_model.063-0.8666666746139526.h5',\n",
       " 'Run Rex the robot.ipynb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# current working directory\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "# ----------------- The Section Responsible for Downloading the Dataset ---------------------\n",
    "\n",
    "\n",
    "SOURCE_PATH = r\"C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\"\n",
    "FILE_DIR = os.path.join(execution_path, \"rex-the-robot-dataset\")\n",
    "DATASET_DIR = os.path.join(execution_path, \"rex-images\")\n",
    "DATASET_TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "DATASET_TEST_DIR = os.path.join(DATASET_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- The Section Responsible for Training ResNet50 on the IdenProf dataset ---------------------\n",
    "\n",
    "# Directory in which to create models\n",
    "save_direc = os.path.join(os.getcwd(), 'rex_models')\n",
    "\n",
    "# Name of model files\n",
    "model_name = 'rex_weight_model.{epoch:03d}-{val_acc}.h5'\n",
    "\n",
    "# Create Directory if it doesn't exist\n",
    "if not os.path.isdir(save_direc):\n",
    "    os.makedirs(save_direc)\n",
    "# Join the directory with the model file\n",
    "modelpath = os.path.join(save_direc, model_name)\n",
    "\n",
    "# Checkpoint to save best model\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for adjusting learning rate and saving dummy file\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Learning Rate Schedule\n",
    "    \"\"\"\n",
    "    # Learning rate is scheduled to be reduced after 80, 120, 160, 180  epochs. Called  automatically  every\n",
    "    #  epoch as part  of  callbacks  during  training.\n",
    "\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 1e-4\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "def resnet_module(input, channel_depth, strided_pool=False):\n",
    "    residual_input = input\n",
    "    stride = 1\n",
    "\n",
    "    if (strided_pool):\n",
    "        stride = 2\n",
    "        residual_input = Conv2D(channel_depth, kernel_size=1, strides=stride, padding=\"same\",\n",
    "                                kernel_initializer=\"he_normal\")(residual_input)\n",
    "        residual_input = BatchNormalization()(residual_input)\n",
    "\n",
    "    input = Conv2D(int(channel_depth / 4), kernel_size=1, strides=stride, padding=\"same\",\n",
    "                   kernel_initializer=\"he_normal\")(input)\n",
    "    input = BatchNormalization()(input)\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    input = Conv2D(int(channel_depth / 4), kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(\n",
    "        input)\n",
    "    input = BatchNormalization()(input)\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    input = Conv2D(channel_depth, kernel_size=1, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(input)\n",
    "    input = BatchNormalization()(input)\n",
    "\n",
    "    input = add([input, residual_input])\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def resnet_first_block_first_module(input, channel_depth):\n",
    "    residual_input = input\n",
    "    stride = 1\n",
    "\n",
    "    residual_input = Conv2D(channel_depth, kernel_size=1, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(\n",
    "        residual_input)\n",
    "    residual_input = BatchNormalization()(residual_input)\n",
    "\n",
    "    input = Conv2D(int(channel_depth / 4), kernel_size=1, strides=stride, padding=\"same\",\n",
    "                   kernel_initializer=\"he_normal\")(input)\n",
    "    input = BatchNormalization()(input)\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    input = Conv2D(int(channel_depth / 4), kernel_size=3, strides=stride, padding=\"same\",\n",
    "                   kernel_initializer=\"he_normal\")(input)\n",
    "    input = BatchNormalization()(input)\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    input = Conv2D(channel_depth, kernel_size=1, strides=stride, padding=\"same\", kernel_initializer=\"he_normal\")(input)\n",
    "    input = BatchNormalization()(input)\n",
    "\n",
    "    input = add([input, residual_input])\n",
    "    input = Activation(\"relu\")(input)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def resnet_block(input, channel_depth, num_layers, strided_pool_first=False):\n",
    "    for i in range(num_layers):\n",
    "        pool = False\n",
    "        if (i == 0 and strided_pool_first):\n",
    "            pool = True\n",
    "        input = resnet_module(input, channel_depth, strided_pool=pool)\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def ResNet50(input_shape, num_classes=2):\n",
    "    input_object = Input(shape=input_shape)\n",
    "    layers = [3, 4, 6, 3]\n",
    "    channel_depths = [256, 512, 1024, 2048]\n",
    "\n",
    "    output = Conv2D(64, kernel_size=7, strides=2, padding=\"same\", kernel_initializer=\"he_normal\")(input_object)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Activation(\"relu\")(output)\n",
    "\n",
    "    output = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(output)\n",
    "    output = resnet_first_block_first_module(output, channel_depths[0])\n",
    "\n",
    "    for i in range(4):\n",
    "        channel_depth = channel_depths[i]\n",
    "        num_layers = layers[i]\n",
    "\n",
    "        strided_pool_first = True\n",
    "        if (i == 0):\n",
    "            strided_pool_first = False\n",
    "            num_layers = num_layers - 1\n",
    "        output = resnet_block(output, channel_depth=channel_depth, num_layers=num_layers,\n",
    "                              strided_pool_first=strided_pool_first)\n",
    "\n",
    "    output = GlobalAvgPool2D()(output)\n",
    "    output = Dense(num_classes)(output)\n",
    "    output = Activation(\"softmax\")(output)\n",
    "\n",
    "    model = Model(inputs=input_object, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    #download_idenprof()\n",
    "\n",
    "    #print(os.listdir(os.path.join(execution_path, \"Rex the robot\")))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.03, decay=1e-4)\n",
    "    batch_size = 5\n",
    "    num_classes = 2\n",
    "    epochs = 100\n",
    "\n",
    "    model = ResNet50((224, 224, 3), num_classes=num_classes)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Using real time Data Augmentation\")\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(DATASET_TRAIN_DIR, target_size=(224, 224),\n",
    "                                                        batch_size=batch_size, class_mode=\"categorical\")\n",
    "    test_generator = test_datagen.flow_from_directory(DATASET_TEST_DIR, target_size=(224, 224), batch_size=batch_size,\n",
    "                                                      class_mode=\"categorical\")\n",
    "\n",
    "    model.fit_generator(train_generator, steps_per_epoch=int(150 / batch_size), epochs=epochs,\n",
    "                        validation_data=test_generator,\n",
    "                        validation_steps=int(30 / batch_size), callbacks=[checkpoint, lr_scheduler]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- The Section Responsible for Inference ---------------------\n",
    "CLASS_INDEX = None\n",
    "\n",
    "MODEL_PATH = os.path.join(execution_path, \"rex_weight_model.063-0.8666666746139526.h5\")\n",
    "JSON_PATH = os.path.join(execution_path, \"rex_model_class.json\")\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x *= (1. / 255)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def decode_predictions(preds, top=5, model_json=\"\"):\n",
    "    global CLASS_INDEX\n",
    "\n",
    "    if CLASS_INDEX is None:\n",
    "        CLASS_INDEX = json.load(open(model_json))\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        for i in top_indices:\n",
    "            each_result = []\n",
    "            each_result.append(CLASS_INDEX[str(i)])\n",
    "            each_result.append(pred[i])\n",
    "            results.append(each_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_inference():\n",
    "    model = ResNet50(input_shape=(224, 224, 3), num_classes=2)\n",
    "    model.load_weights(MODEL_PATH)\n",
    "\n",
    "    picture = os.path.join(execution_path, \"interference-image.jpg\")\n",
    "\n",
    "    image_to_predict = image.load_img(picture, target_size=(\n",
    "        224, 224))\n",
    "    image_to_predict = image.img_to_array(image_to_predict, data_format=\"channels_last\")\n",
    "    image_to_predict = np.expand_dims(image_to_predict, axis=0)\n",
    "\n",
    "    image_to_predict = preprocess_input(image_to_predict)\n",
    "\n",
    "    prediction = model.predict(x=image_to_predict, steps=1)\n",
    "\n",
    "    predictiondata = decode_predictions(prediction, top=int(5), model_json=JSON_PATH)\n",
    "\n",
    "    for result in predictiondata:\n",
    "        print(str(result[0]), \" : \", str(result[1] * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lizzi\\Anaconda3\\envs\\thesis2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 112, 112, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 55, 55, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 55, 55, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 55, 55, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 55, 55, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 55, 55, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 55, 55, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 55, 55, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 55, 55, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 55, 55, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 55, 55, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 256)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 55, 55, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 55, 55, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 55, 55, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 55, 55, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 55, 55, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 55, 55, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 55, 55, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 55, 55, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 55, 55, 256)  0           batch_normalization_8[0][0]      \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 55, 55, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 55, 55, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 55, 55, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 55, 55, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 55, 55, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 55, 55, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 55, 55, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 55, 55, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 55, 55, 256)  0           batch_normalization_11[0][0]     \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 55, 55, 256)  0           add_3[0][0]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 28, 28, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 512)  2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           batch_normalization_18[0][0]     \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 28, 28, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 28, 28, 512)  2048        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           batch_normalization_21[0][0]     \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 28, 28, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 28, 28, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_24 (BatchNo (None, 28, 28, 512)  2048        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 28, 28, 512)  0           batch_normalization_24[0][0]     \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 256)  1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 256)  1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 256)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 1024) 4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 1024) 4096        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_28[0][0]     \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 256)  1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 256)  1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 256)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 1024) 4096        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_31[0][0]     \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 256)  1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 256)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 1024) 4096        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_34[0][0]     \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_36 (BatchNo (None, 14, 14, 256)  1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 14, 14, 1024) 4096        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_37[0][0]     \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 14, 14, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 1024) 4096        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_40[0][0]     \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 14, 14, 1024) 4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_43[0][0]     \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 512)    2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 512)    2048        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 512)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 2048)   8192        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 2048)   8192        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_47[0][0]     \n",
      "                                                                 batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_48 (BatchNo (None, 7, 7, 512)    2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 512)    2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 512)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 2048)   8192        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_50[0][0]     \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 512)    2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 512)    2048        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 512)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 7, 7, 2048)   8192        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_53[0][0]     \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            4098        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 2)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Using real time Data Augmentation\n",
      "Found 150 images belonging to 2 classes.\n",
      "Found 30 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\lizzi\\Anaconda3\\envs\\thesis2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 6:20 - loss: 0.4920 - acc: 0.800 - ETA: 3:06 - loss: 1.7298 - acc: 0.600 - ETA: 2:00 - loss: 2.6180 - acc: 0.600 - ETA: 1:28 - loss: 3.0559 - acc: 0.550 - ETA: 1:08 - loss: 2.5553 - acc: 0.560 - ETA: 55s - loss: 3.0100 - acc: 0.533 - ETA: 46s - loss: 3.4680 - acc: 0.51 - ETA: 39s - loss: 3.7889 - acc: 0.50 - ETA: 33s - loss: 3.9576 - acc: 0.46 - ETA: 29s - loss: 3.6733 - acc: 0.48 - ETA: 25s - loss: 3.7416 - acc: 0.43 - ETA: 22s - loss: 3.6251 - acc: 0.43 - ETA: 19s - loss: 3.4240 - acc: 0.44 - ETA: 17s - loss: 3.2867 - acc: 0.45 - ETA: 15s - loss: 3.1383 - acc: 0.45 - ETA: 13s - loss: 2.9845 - acc: 0.47 - ETA: 11s - loss: 2.8623 - acc: 0.49 - ETA: 10s - loss: 2.8031 - acc: 0.50 - ETA: 9s - loss: 2.6912 - acc: 0.5053 - ETA: 8s - loss: 2.5845 - acc: 0.500 - ETA: 6s - loss: 2.4823 - acc: 0.504 - ETA: 5s - loss: 2.4212 - acc: 0.490 - ETA: 5s - loss: 2.3262 - acc: 0.513 - ETA: 4s - loss: 2.3089 - acc: 0.516 - ETA: 3s - loss: 2.2394 - acc: 0.528 - ETA: 2s - loss: 2.2170 - acc: 0.523 - ETA: 1s - loss: 2.1531 - acc: 0.525 - ETA: 1s - loss: 2.1272 - acc: 0.521 - ETA: 0s - loss: 2.0922 - acc: 0.517 - 19s 619ms/step - loss: 2.0421 - acc: 0.5267 - val_loss: 7.3594 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50000, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.001-0.5000000124176344.h5\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.4549 - acc: 0.600 - ETA: 4s - loss: 0.4926 - acc: 0.700 - ETA: 4s - loss: 0.6850 - acc: 0.600 - ETA: 4s - loss: 0.6323 - acc: 0.650 - ETA: 3s - loss: 0.7402 - acc: 0.600 - ETA: 3s - loss: 0.7314 - acc: 0.633 - ETA: 3s - loss: 0.7454 - acc: 0.657 - ETA: 3s - loss: 0.6795 - acc: 0.700 - ETA: 3s - loss: 0.6435 - acc: 0.711 - ETA: 3s - loss: 0.6219 - acc: 0.700 - ETA: 2s - loss: 0.6049 - acc: 0.709 - ETA: 2s - loss: 0.7986 - acc: 0.700 - ETA: 2s - loss: 0.7698 - acc: 0.707 - ETA: 2s - loss: 0.9170 - acc: 0.700 - ETA: 2s - loss: 0.8756 - acc: 0.706 - ETA: 2s - loss: 0.8456 - acc: 0.712 - ETA: 2s - loss: 0.8044 - acc: 0.729 - ETA: 1s - loss: 0.7718 - acc: 0.733 - ETA: 1s - loss: 0.8063 - acc: 0.726 - ETA: 1s - loss: 0.8024 - acc: 0.720 - ETA: 1s - loss: 0.7748 - acc: 0.723 - ETA: 1s - loss: 0.8668 - acc: 0.700 - ETA: 1s - loss: 0.8694 - acc: 0.704 - ETA: 0s - loss: 0.8572 - acc: 0.708 - ETA: 0s - loss: 0.8271 - acc: 0.720 - ETA: 0s - loss: 0.8152 - acc: 0.715 - ETA: 0s - loss: 0.9902 - acc: 0.696 - ETA: 0s - loss: 1.1112 - acc: 0.678 - ETA: 0s - loss: 1.1604 - acc: 0.675 - 5s 164ms/step - loss: 1.1631 - acc: 0.6600 - val_loss: 6.2384 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50000\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.7866 - acc: 0.600 - ETA: 4s - loss: 0.5820 - acc: 0.700 - ETA: 4s - loss: 0.5808 - acc: 0.666 - ETA: 4s - loss: 0.5814 - acc: 0.650 - ETA: 3s - loss: 0.6537 - acc: 0.560 - ETA: 3s - loss: 0.6947 - acc: 0.500 - ETA: 3s - loss: 0.7342 - acc: 0.485 - ETA: 3s - loss: 0.8418 - acc: 0.425 - ETA: 3s - loss: 0.8719 - acc: 0.400 - ETA: 3s - loss: 0.8551 - acc: 0.440 - ETA: 2s - loss: 0.8401 - acc: 0.454 - ETA: 2s - loss: 0.8887 - acc: 0.433 - ETA: 2s - loss: 0.8945 - acc: 0.446 - ETA: 2s - loss: 0.9037 - acc: 0.442 - ETA: 2s - loss: 0.8694 - acc: 0.466 - ETA: 2s - loss: 0.8592 - acc: 0.475 - ETA: 2s - loss: 0.8788 - acc: 0.470 - ETA: 1s - loss: 0.8408 - acc: 0.500 - ETA: 1s - loss: 0.8449 - acc: 0.505 - ETA: 1s - loss: 0.8648 - acc: 0.510 - ETA: 1s - loss: 0.9458 - acc: 0.504 - ETA: 1s - loss: 0.9563 - acc: 0.509 - ETA: 1s - loss: 0.9429 - acc: 0.513 - ETA: 0s - loss: 0.9498 - acc: 0.525 - ETA: 0s - loss: 0.9684 - acc: 0.520 - ETA: 0s - loss: 0.9787 - acc: 0.523 - ETA: 0s - loss: 0.9872 - acc: 0.518 - ETA: 0s - loss: 0.9814 - acc: 0.514 - ETA: 0s - loss: 0.9716 - acc: 0.517 - 5s 165ms/step - loss: 0.9518 - acc: 0.5333 - val_loss: 3.3136 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.50000\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2557 - acc: 1.000 - ETA: 4s - loss: 0.2070 - acc: 1.000 - ETA: 4s - loss: 0.2313 - acc: 1.000 - ETA: 4s - loss: 0.2569 - acc: 0.950 - ETA: 3s - loss: 0.2742 - acc: 0.920 - ETA: 3s - loss: 0.4928 - acc: 0.800 - ETA: 3s - loss: 0.4569 - acc: 0.828 - ETA: 3s - loss: 0.4962 - acc: 0.800 - ETA: 3s - loss: 0.5374 - acc: 0.800 - ETA: 3s - loss: 0.5198 - acc: 0.800 - ETA: 2s - loss: 0.5020 - acc: 0.800 - ETA: 2s - loss: 0.5916 - acc: 0.766 - ETA: 2s - loss: 0.6759 - acc: 0.738 - ETA: 2s - loss: 0.7287 - acc: 0.742 - ETA: 2s - loss: 0.7800 - acc: 0.733 - ETA: 2s - loss: 0.7559 - acc: 0.737 - ETA: 2s - loss: 0.7311 - acc: 0.741 - ETA: 1s - loss: 0.7097 - acc: 0.755 - ETA: 1s - loss: 0.6846 - acc: 0.768 - ETA: 1s - loss: 0.6791 - acc: 0.770 - ETA: 1s - loss: 0.7048 - acc: 0.771 - ETA: 1s - loss: 0.6959 - acc: 0.763 - ETA: 1s - loss: 0.6779 - acc: 0.773 - ETA: 0s - loss: 0.8069 - acc: 0.750 - ETA: 0s - loss: 0.7850 - acc: 0.752 - ETA: 0s - loss: 0.8677 - acc: 0.746 - ETA: 0s - loss: 0.8462 - acc: 0.755 - ETA: 0s - loss: 0.8303 - acc: 0.757 - ETA: 0s - loss: 0.8035 - acc: 0.765 - 5s 164ms/step - loss: 0.7893 - acc: 0.7600 - val_loss: 2.3441 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.50000 to 0.70000, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.004-0.7000000178813934.h5\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.7462 - acc: 0.800 - ETA: 4s - loss: 0.6130 - acc: 0.800 - ETA: 4s - loss: 0.7076 - acc: 0.733 - ETA: 4s - loss: 0.6587 - acc: 0.700 - ETA: 3s - loss: 1.0892 - acc: 0.560 - ETA: 3s - loss: 1.0808 - acc: 0.533 - ETA: 3s - loss: 0.9651 - acc: 0.571 - ETA: 3s - loss: 0.9816 - acc: 0.525 - ETA: 3s - loss: 0.8994 - acc: 0.555 - ETA: 3s - loss: 0.8533 - acc: 0.560 - ETA: 2s - loss: 0.7982 - acc: 0.600 - ETA: 2s - loss: 0.8056 - acc: 0.583 - ETA: 2s - loss: 0.7640 - acc: 0.600 - ETA: 2s - loss: 0.7493 - acc: 0.600 - ETA: 2s - loss: 0.7279 - acc: 0.613 - ETA: 2s - loss: 0.6949 - acc: 0.637 - ETA: 2s - loss: 0.6706 - acc: 0.658 - ETA: 1s - loss: 0.6362 - acc: 0.677 - ETA: 1s - loss: 0.6244 - acc: 0.684 - ETA: 1s - loss: 0.7006 - acc: 0.680 - ETA: 1s - loss: 0.6762 - acc: 0.695 - ETA: 1s - loss: 0.6672 - acc: 0.690 - ETA: 1s - loss: 0.6633 - acc: 0.687 - ETA: 0s - loss: 0.6482 - acc: 0.700 - ETA: 0s - loss: 0.6990 - acc: 0.696 - ETA: 0s - loss: 0.7529 - acc: 0.684 - ETA: 0s - loss: 0.7345 - acc: 0.688 - ETA: 0s - loss: 0.7462 - acc: 0.685 - ETA: 0s - loss: 0.7288 - acc: 0.696 - 5s 164ms/step - loss: 0.7104 - acc: 0.7067 - val_loss: 2.9145 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70000\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.2858 - acc: 0.200 - ETA: 4s - loss: 0.7643 - acc: 0.600 - ETA: 4s - loss: 0.6008 - acc: 0.666 - ETA: 4s - loss: 0.5183 - acc: 0.750 - ETA: 3s - loss: 0.5352 - acc: 0.720 - ETA: 3s - loss: 0.5356 - acc: 0.733 - ETA: 3s - loss: 0.4850 - acc: 0.771 - ETA: 3s - loss: 0.4594 - acc: 0.775 - ETA: 3s - loss: 0.4602 - acc: 0.777 - ETA: 3s - loss: 0.4497 - acc: 0.780 - ETA: 2s - loss: 0.4273 - acc: 0.800 - ETA: 2s - loss: 0.4187 - acc: 0.800 - ETA: 2s - loss: 0.5369 - acc: 0.784 - ETA: 2s - loss: 0.5101 - acc: 0.785 - ETA: 2s - loss: 0.4912 - acc: 0.800 - ETA: 2s - loss: 0.5269 - acc: 0.775 - ETA: 2s - loss: 0.5354 - acc: 0.764 - ETA: 1s - loss: 0.5970 - acc: 0.744 - ETA: 1s - loss: 0.5956 - acc: 0.747 - ETA: 1s - loss: 0.6180 - acc: 0.750 - ETA: 1s - loss: 0.6254 - acc: 0.752 - ETA: 1s - loss: 0.6383 - acc: 0.754 - ETA: 1s - loss: 0.6293 - acc: 0.756 - ETA: 0s - loss: 0.6095 - acc: 0.766 - ETA: 0s - loss: 0.6165 - acc: 0.768 - ETA: 0s - loss: 0.6060 - acc: 0.769 - ETA: 0s - loss: 0.5960 - acc: 0.770 - ETA: 0s - loss: 0.6015 - acc: 0.764 - ETA: 0s - loss: 0.5854 - acc: 0.772 - 5s 164ms/step - loss: 0.5711 - acc: 0.7800 - val_loss: 2.6417 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70000\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0901 - acc: 1.000 - ETA: 4s - loss: 0.1921 - acc: 0.900 - ETA: 4s - loss: 0.1344 - acc: 0.933 - ETA: 4s - loss: 0.1928 - acc: 0.900 - ETA: 3s - loss: 0.1685 - acc: 0.920 - ETA: 3s - loss: 0.3338 - acc: 0.866 - ETA: 3s - loss: 0.4319 - acc: 0.857 - ETA: 3s - loss: 0.5229 - acc: 0.850 - ETA: 3s - loss: 0.4741 - acc: 0.866 - ETA: 3s - loss: 0.4528 - acc: 0.860 - ETA: 3s - loss: 0.5395 - acc: 0.854 - ETA: 2s - loss: 0.5459 - acc: 0.833 - ETA: 2s - loss: 0.5615 - acc: 0.815 - ETA: 2s - loss: 0.5444 - acc: 0.814 - ETA: 2s - loss: 0.5221 - acc: 0.826 - ETA: 2s - loss: 0.5704 - acc: 0.825 - ETA: 2s - loss: 0.5540 - acc: 0.823 - ETA: 1s - loss: 0.5445 - acc: 0.822 - ETA: 1s - loss: 0.5534 - acc: 0.800 - ETA: 1s - loss: 0.5369 - acc: 0.810 - ETA: 1s - loss: 0.5267 - acc: 0.809 - ETA: 1s - loss: 0.5193 - acc: 0.809 - ETA: 1s - loss: 0.5215 - acc: 0.808 - ETA: 0s - loss: 0.5190 - acc: 0.808 - ETA: 0s - loss: 0.5030 - acc: 0.816 - ETA: 0s - loss: 0.5270 - acc: 0.807 - ETA: 0s - loss: 0.5141 - acc: 0.814 - ETA: 0s - loss: 0.5388 - acc: 0.807 - ETA: 0s - loss: 0.5311 - acc: 0.806 - 5s 165ms/step - loss: 0.5617 - acc: 0.8067 - val_loss: 3.1509 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70000\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.0258 - acc: 0.600 - ETA: 4s - loss: 1.0421 - acc: 0.800 - ETA: 4s - loss: 0.7909 - acc: 0.800 - ETA: 4s - loss: 0.7542 - acc: 0.750 - ETA: 3s - loss: 0.6220 - acc: 0.800 - ETA: 3s - loss: 0.6192 - acc: 0.800 - ETA: 3s - loss: 0.6463 - acc: 0.800 - ETA: 3s - loss: 0.5890 - acc: 0.825 - ETA: 3s - loss: 0.5853 - acc: 0.822 - ETA: 3s - loss: 0.5381 - acc: 0.840 - ETA: 2s - loss: 0.4984 - acc: 0.854 - ETA: 2s - loss: 0.4878 - acc: 0.850 - ETA: 2s - loss: 0.4948 - acc: 0.830 - ETA: 2s - loss: 0.4795 - acc: 0.828 - ETA: 2s - loss: 0.5049 - acc: 0.800 - ETA: 2s - loss: 0.5125 - acc: 0.800 - ETA: 2s - loss: 0.5440 - acc: 0.800 - ETA: 1s - loss: 0.5852 - acc: 0.788 - ETA: 1s - loss: 0.5870 - acc: 0.789 - ETA: 1s - loss: 0.5760 - acc: 0.790 - ETA: 1s - loss: 0.5553 - acc: 0.800 - ETA: 1s - loss: 0.5321 - acc: 0.809 - ETA: 1s - loss: 0.5281 - acc: 0.808 - ETA: 0s - loss: 0.5345 - acc: 0.800 - ETA: 0s - loss: 0.5174 - acc: 0.808 - ETA: 0s - loss: 0.5271 - acc: 0.800 - ETA: 0s - loss: 0.5138 - acc: 0.800 - ETA: 0s - loss: 0.4978 - acc: 0.807 - ETA: 0s - loss: 0.4830 - acc: 0.813 - 5s 164ms/step - loss: 0.4742 - acc: 0.8133 - val_loss: 1.6676 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70000\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 1.3705 - acc: 0.800 - ETA: 4s - loss: 1.4310 - acc: 0.700 - ETA: 4s - loss: 1.0105 - acc: 0.733 - ETA: 4s - loss: 1.1004 - acc: 0.750 - ETA: 3s - loss: 0.9171 - acc: 0.760 - ETA: 3s - loss: 0.7966 - acc: 0.800 - ETA: 3s - loss: 0.7306 - acc: 0.800 - ETA: 3s - loss: 0.8445 - acc: 0.725 - ETA: 3s - loss: 0.8036 - acc: 0.711 - ETA: 3s - loss: 0.8215 - acc: 0.700 - ETA: 2s - loss: 0.7557 - acc: 0.727 - ETA: 2s - loss: 0.7248 - acc: 0.733 - ETA: 2s - loss: 0.7111 - acc: 0.723 - ETA: 2s - loss: 0.7333 - acc: 0.714 - ETA: 2s - loss: 0.6885 - acc: 0.733 - ETA: 2s - loss: 0.6513 - acc: 0.750 - ETA: 2s - loss: 0.6500 - acc: 0.752 - ETA: 1s - loss: 0.6711 - acc: 0.755 - ETA: 1s - loss: 0.6406 - acc: 0.768 - ETA: 1s - loss: 0.7020 - acc: 0.760 - ETA: 1s - loss: 0.6704 - acc: 0.771 - ETA: 1s - loss: 0.7118 - acc: 0.763 - ETA: 1s - loss: 0.7216 - acc: 0.756 - ETA: 0s - loss: 0.6967 - acc: 0.766 - ETA: 0s - loss: 0.7223 - acc: 0.768 - ETA: 0s - loss: 0.6949 - acc: 0.776 - ETA: 0s - loss: 0.6842 - acc: 0.777 - ETA: 0s - loss: 0.6616 - acc: 0.785 - ETA: 0s - loss: 0.6408 - acc: 0.793 - 5s 165ms/step - loss: 0.6211 - acc: 0.8000 - val_loss: 0.7010 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.70000 to 0.76667, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.009-0.7666666805744171.h5\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1110 - acc: 1.000 - ETA: 4s - loss: 0.5538 - acc: 0.800 - ETA: 4s - loss: 0.5418 - acc: 0.800 - ETA: 4s - loss: 0.5285 - acc: 0.800 - ETA: 3s - loss: 0.5437 - acc: 0.800 - ETA: 3s - loss: 0.5274 - acc: 0.766 - ETA: 3s - loss: 0.6140 - acc: 0.742 - ETA: 3s - loss: 0.5632 - acc: 0.775 - ETA: 3s - loss: 0.5126 - acc: 0.800 - ETA: 3s - loss: 0.5721 - acc: 0.760 - ETA: 2s - loss: 0.5365 - acc: 0.781 - ETA: 2s - loss: 0.5382 - acc: 0.766 - ETA: 2s - loss: 0.5060 - acc: 0.784 - ETA: 2s - loss: 0.5032 - acc: 0.785 - ETA: 2s - loss: 0.4851 - acc: 0.800 - ETA: 2s - loss: 0.4615 - acc: 0.812 - ETA: 2s - loss: 0.4385 - acc: 0.823 - ETA: 1s - loss: 0.4194 - acc: 0.833 - ETA: 1s - loss: 0.4071 - acc: 0.842 - ETA: 1s - loss: 0.3938 - acc: 0.850 - ETA: 1s - loss: 0.3853 - acc: 0.847 - ETA: 1s - loss: 0.4008 - acc: 0.836 - ETA: 1s - loss: 0.4343 - acc: 0.826 - ETA: 0s - loss: 0.4176 - acc: 0.833 - ETA: 0s - loss: 0.4043 - acc: 0.840 - ETA: 0s - loss: 0.4129 - acc: 0.823 - ETA: 0s - loss: 0.4040 - acc: 0.829 - ETA: 0s - loss: 0.3942 - acc: 0.835 - ETA: 0s - loss: 0.3886 - acc: 0.841 - 5s 165ms/step - loss: 0.3818 - acc: 0.8400 - val_loss: 0.6601 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76667 to 0.80000, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.010-0.800000011920929.h5\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2113 - acc: 0.800 - ETA: 4s - loss: 0.1282 - acc: 0.900 - ETA: 4s - loss: 0.2713 - acc: 0.800 - ETA: 4s - loss: 0.2315 - acc: 0.850 - ETA: 3s - loss: 0.1913 - acc: 0.880 - ETA: 3s - loss: 0.1994 - acc: 0.866 - ETA: 3s - loss: 0.1824 - acc: 0.885 - ETA: 3s - loss: 0.1856 - acc: 0.900 - ETA: 3s - loss: 0.1845 - acc: 0.911 - ETA: 3s - loss: 0.1772 - acc: 0.920 - ETA: 2s - loss: 0.1718 - acc: 0.927 - ETA: 2s - loss: 0.2853 - acc: 0.883 - ETA: 2s - loss: 0.2658 - acc: 0.892 - ETA: 2s - loss: 0.2906 - acc: 0.871 - ETA: 2s - loss: 0.2758 - acc: 0.880 - ETA: 2s - loss: 0.2787 - acc: 0.875 - ETA: 2s - loss: 0.2764 - acc: 0.870 - ETA: 1s - loss: 0.2709 - acc: 0.877 - ETA: 1s - loss: 0.2599 - acc: 0.884 - ETA: 1s - loss: 0.2578 - acc: 0.880 - ETA: 1s - loss: 0.2591 - acc: 0.876 - ETA: 1s - loss: 0.2652 - acc: 0.872 - ETA: 1s - loss: 0.2752 - acc: 0.869 - ETA: 0s - loss: 0.3296 - acc: 0.850 - ETA: 0s - loss: 0.3185 - acc: 0.856 - ETA: 0s - loss: 0.3245 - acc: 0.853 - ETA: 0s - loss: 0.3164 - acc: 0.859 - ETA: 0s - loss: 0.3085 - acc: 0.864 - ETA: 0s - loss: 0.2999 - acc: 0.869 - 5s 164ms/step - loss: 0.2951 - acc: 0.8733 - val_loss: 1.4436 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80000\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2019 - acc: 1.000 - ETA: 4s - loss: 0.1967 - acc: 0.900 - ETA: 4s - loss: 0.1524 - acc: 0.933 - ETA: 4s - loss: 0.1452 - acc: 0.950 - ETA: 3s - loss: 0.1579 - acc: 0.960 - ETA: 3s - loss: 0.1400 - acc: 0.966 - ETA: 3s - loss: 0.1281 - acc: 0.971 - ETA: 3s - loss: 0.1641 - acc: 0.950 - ETA: 3s - loss: 0.2505 - acc: 0.888 - ETA: 3s - loss: 0.2354 - acc: 0.900 - ETA: 3s - loss: 0.2156 - acc: 0.909 - ETA: 2s - loss: 0.2232 - acc: 0.900 - ETA: 2s - loss: 0.2083 - acc: 0.907 - ETA: 2s - loss: 0.2040 - acc: 0.914 - ETA: 2s - loss: 0.1909 - acc: 0.920 - ETA: 2s - loss: 0.2082 - acc: 0.912 - ETA: 2s - loss: 0.2126 - acc: 0.905 - ETA: 1s - loss: 0.2840 - acc: 0.888 - ETA: 1s - loss: 0.2961 - acc: 0.884 - ETA: 1s - loss: 0.2996 - acc: 0.880 - ETA: 1s - loss: 0.3214 - acc: 0.866 - ETA: 1s - loss: 0.3326 - acc: 0.854 - ETA: 1s - loss: 0.3272 - acc: 0.860 - ETA: 0s - loss: 0.3146 - acc: 0.866 - ETA: 0s - loss: 0.3200 - acc: 0.864 - ETA: 0s - loss: 0.3164 - acc: 0.869 - ETA: 0s - loss: 0.3198 - acc: 0.866 - ETA: 0s - loss: 0.3188 - acc: 0.871 - ETA: 0s - loss: 0.3193 - acc: 0.869 - 5s 165ms/step - loss: 0.3348 - acc: 0.8600 - val_loss: 0.7537 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80000\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0429 - acc: 1.000 - ETA: 4s - loss: 0.0681 - acc: 1.000 - ETA: 4s - loss: 0.0619 - acc: 1.000 - ETA: 4s - loss: 0.0733 - acc: 1.000 - ETA: 3s - loss: 0.1965 - acc: 0.960 - ETA: 3s - loss: 0.1836 - acc: 0.966 - ETA: 3s - loss: 0.1658 - acc: 0.971 - ETA: 3s - loss: 0.1546 - acc: 0.975 - ETA: 3s - loss: 0.1813 - acc: 0.955 - ETA: 3s - loss: 0.1955 - acc: 0.960 - ETA: 2s - loss: 0.2075 - acc: 0.945 - ETA: 2s - loss: 0.2100 - acc: 0.933 - ETA: 2s - loss: 0.2188 - acc: 0.923 - ETA: 2s - loss: 0.2083 - acc: 0.928 - ETA: 2s - loss: 0.2139 - acc: 0.920 - ETA: 2s - loss: 0.2020 - acc: 0.925 - ETA: 2s - loss: 0.1952 - acc: 0.929 - ETA: 1s - loss: 0.1933 - acc: 0.933 - ETA: 1s - loss: 0.1855 - acc: 0.936 - ETA: 1s - loss: 0.1850 - acc: 0.930 - ETA: 1s - loss: 0.1857 - acc: 0.933 - ETA: 1s - loss: 0.2186 - acc: 0.909 - ETA: 1s - loss: 0.2112 - acc: 0.913 - ETA: 0s - loss: 0.2182 - acc: 0.908 - ETA: 0s - loss: 0.2123 - acc: 0.912 - ETA: 0s - loss: 0.2070 - acc: 0.915 - ETA: 0s - loss: 0.2049 - acc: 0.918 - ETA: 0s - loss: 0.1979 - acc: 0.921 - ETA: 0s - loss: 0.2050 - acc: 0.917 - 5s 164ms/step - loss: 0.2205 - acc: 0.9133 - val_loss: 1.2043 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80000\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0309 - acc: 1.000 - ETA: 4s - loss: 0.1930 - acc: 0.900 - ETA: 4s - loss: 0.1665 - acc: 0.933 - ETA: 4s - loss: 0.1276 - acc: 0.950 - ETA: 3s - loss: 0.1104 - acc: 0.960 - ETA: 3s - loss: 0.2769 - acc: 0.833 - ETA: 3s - loss: 0.2549 - acc: 0.857 - ETA: 3s - loss: 0.2319 - acc: 0.875 - ETA: 3s - loss: 0.4097 - acc: 0.800 - ETA: 3s - loss: 0.3711 - acc: 0.820 - ETA: 3s - loss: 0.3835 - acc: 0.818 - ETA: 2s - loss: 0.3649 - acc: 0.833 - ETA: 2s - loss: 0.3469 - acc: 0.846 - ETA: 2s - loss: 0.3261 - acc: 0.857 - ETA: 2s - loss: 0.3083 - acc: 0.866 - ETA: 2s - loss: 0.3309 - acc: 0.862 - ETA: 2s - loss: 0.3262 - acc: 0.870 - ETA: 1s - loss: 0.3840 - acc: 0.844 - ETA: 1s - loss: 0.3688 - acc: 0.852 - ETA: 1s - loss: 0.3699 - acc: 0.850 - ETA: 1s - loss: 0.3576 - acc: 0.857 - ETA: 1s - loss: 0.3684 - acc: 0.854 - ETA: 1s - loss: 0.3823 - acc: 0.834 - ETA: 0s - loss: 0.3792 - acc: 0.833 - ETA: 0s - loss: 0.3992 - acc: 0.824 - ETA: 0s - loss: 0.4015 - acc: 0.823 - ETA: 0s - loss: 0.3890 - acc: 0.829 - ETA: 0s - loss: 0.3874 - acc: 0.828 - ETA: 0s - loss: 0.3865 - acc: 0.827 - 5s 165ms/step - loss: 0.3794 - acc: 0.8333 - val_loss: 2.0096 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80000\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.2843 - acc: 0.800 - ETA: 4s - loss: 0.4707 - acc: 0.800 - ETA: 4s - loss: 0.4697 - acc: 0.800 - ETA: 4s - loss: 0.4293 - acc: 0.750 - ETA: 3s - loss: 0.3551 - acc: 0.800 - ETA: 3s - loss: 0.4327 - acc: 0.800 - ETA: 3s - loss: 0.3821 - acc: 0.828 - ETA: 3s - loss: 0.3350 - acc: 0.850 - ETA: 3s - loss: 0.4460 - acc: 0.800 - ETA: 3s - loss: 0.4403 - acc: 0.800 - ETA: 2s - loss: 0.4046 - acc: 0.818 - ETA: 2s - loss: 0.3879 - acc: 0.816 - ETA: 2s - loss: 0.3873 - acc: 0.815 - ETA: 2s - loss: 0.4096 - acc: 0.814 - ETA: 2s - loss: 0.4016 - acc: 0.813 - ETA: 2s - loss: 0.3801 - acc: 0.825 - ETA: 2s - loss: 0.4336 - acc: 0.811 - ETA: 1s - loss: 0.4260 - acc: 0.811 - ETA: 1s - loss: 0.4886 - acc: 0.810 - ETA: 1s - loss: 0.4752 - acc: 0.810 - ETA: 1s - loss: 0.4572 - acc: 0.819 - ETA: 1s - loss: 0.4854 - acc: 0.809 - ETA: 1s - loss: 0.4675 - acc: 0.817 - ETA: 0s - loss: 0.4517 - acc: 0.825 - ETA: 0s - loss: 0.4512 - acc: 0.816 - ETA: 0s - loss: 0.4764 - acc: 0.807 - ETA: 0s - loss: 0.4649 - acc: 0.807 - ETA: 0s - loss: 0.4499 - acc: 0.814 - ETA: 0s - loss: 0.4438 - acc: 0.813 - 5s 164ms/step - loss: 0.4423 - acc: 0.8133 - val_loss: 0.9378 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80000\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.5039 - acc: 0.800 - ETA: 4s - loss: 0.5586 - acc: 0.800 - ETA: 4s - loss: 0.4359 - acc: 0.866 - ETA: 4s - loss: 0.4925 - acc: 0.800 - ETA: 3s - loss: 0.4149 - acc: 0.840 - ETA: 3s - loss: 0.3783 - acc: 0.866 - ETA: 3s - loss: 0.3311 - acc: 0.885 - ETA: 3s - loss: 0.2917 - acc: 0.900 - ETA: 3s - loss: 0.4236 - acc: 0.866 - ETA: 3s - loss: 0.3878 - acc: 0.880 - ETA: 2s - loss: 0.3727 - acc: 0.872 - ETA: 2s - loss: 0.3501 - acc: 0.883 - ETA: 2s - loss: 0.3729 - acc: 0.876 - ETA: 2s - loss: 0.3533 - acc: 0.885 - ETA: 2s - loss: 0.3466 - acc: 0.893 - ETA: 2s - loss: 0.3488 - acc: 0.887 - ETA: 2s - loss: 0.3316 - acc: 0.894 - ETA: 1s - loss: 0.3231 - acc: 0.900 - ETA: 1s - loss: 0.3295 - acc: 0.894 - ETA: 1s - loss: 0.3873 - acc: 0.890 - ETA: 1s - loss: 0.3720 - acc: 0.895 - ETA: 1s - loss: 0.3568 - acc: 0.900 - ETA: 1s - loss: 0.3456 - acc: 0.904 - ETA: 0s - loss: 0.3323 - acc: 0.908 - ETA: 0s - loss: 0.3333 - acc: 0.904 - ETA: 0s - loss: 0.3785 - acc: 0.892 - ETA: 0s - loss: 0.3672 - acc: 0.896 - ETA: 0s - loss: 0.3559 - acc: 0.900 - ETA: 0s - loss: 0.3627 - acc: 0.896 - 5s 164ms/step - loss: 0.3662 - acc: 0.8933 - val_loss: 0.7614 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.80000 to 0.83333, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.016-0.8333333432674408.h5\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1359 - acc: 1.000 - ETA: 4s - loss: 0.1924 - acc: 0.900 - ETA: 4s - loss: 0.1545 - acc: 0.933 - ETA: 4s - loss: 0.1234 - acc: 0.950 - ETA: 3s - loss: 0.1046 - acc: 0.960 - ETA: 3s - loss: 0.1108 - acc: 0.966 - ETA: 3s - loss: 0.1031 - acc: 0.971 - ETA: 3s - loss: 0.1084 - acc: 0.975 - ETA: 3s - loss: 0.1087 - acc: 0.977 - ETA: 3s - loss: 0.1063 - acc: 0.980 - ETA: 2s - loss: 0.1494 - acc: 0.945 - ETA: 2s - loss: 0.1427 - acc: 0.950 - ETA: 2s - loss: 0.1471 - acc: 0.938 - ETA: 2s - loss: 0.2101 - acc: 0.914 - ETA: 2s - loss: 0.2551 - acc: 0.880 - ETA: 2s - loss: 0.2415 - acc: 0.887 - ETA: 2s - loss: 0.2445 - acc: 0.882 - ETA: 1s - loss: 0.2522 - acc: 0.877 - ETA: 1s - loss: 0.2433 - acc: 0.884 - ETA: 1s - loss: 0.2393 - acc: 0.880 - ETA: 1s - loss: 0.2477 - acc: 0.866 - ETA: 1s - loss: 0.2566 - acc: 0.863 - ETA: 1s - loss: 0.2595 - acc: 0.860 - ETA: 0s - loss: 0.2509 - acc: 0.866 - ETA: 0s - loss: 0.2448 - acc: 0.872 - ETA: 0s - loss: 0.2444 - acc: 0.869 - ETA: 0s - loss: 0.2372 - acc: 0.874 - ETA: 0s - loss: 0.2534 - acc: 0.864 - ETA: 0s - loss: 0.2469 - acc: 0.869 - 5s 164ms/step - loss: 0.2401 - acc: 0.8733 - val_loss: 0.5967 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83333\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 5s - loss: 0.0177 - acc: 1.000 - ETA: 4s - loss: 0.0199 - acc: 1.000 - ETA: 4s - loss: 0.1274 - acc: 0.933 - ETA: 4s - loss: 0.1431 - acc: 0.900 - ETA: 4s - loss: 0.1320 - acc: 0.920 - ETA: 3s - loss: 0.1124 - acc: 0.933 - ETA: 3s - loss: 0.1026 - acc: 0.942 - ETA: 3s - loss: 0.0951 - acc: 0.950 - ETA: 3s - loss: 0.4844 - acc: 0.844 - ETA: 3s - loss: 0.4392 - acc: 0.860 - ETA: 3s - loss: 0.4044 - acc: 0.872 - ETA: 2s - loss: 0.3734 - acc: 0.883 - ETA: 2s - loss: 0.3475 - acc: 0.892 - ETA: 2s - loss: 0.3951 - acc: 0.857 - ETA: 2s - loss: 0.3974 - acc: 0.853 - ETA: 2s - loss: 0.3760 - acc: 0.862 - ETA: 2s - loss: 0.3559 - acc: 0.870 - ETA: 1s - loss: 0.4121 - acc: 0.866 - ETA: 1s - loss: 0.3933 - acc: 0.873 - ETA: 1s - loss: 0.4098 - acc: 0.860 - ETA: 1s - loss: 0.3919 - acc: 0.866 - ETA: 1s - loss: 0.3796 - acc: 0.872 - ETA: 1s - loss: 0.3666 - acc: 0.878 - ETA: 0s - loss: 0.3552 - acc: 0.883 - ETA: 0s - loss: 0.3421 - acc: 0.888 - ETA: 0s - loss: 0.3298 - acc: 0.892 - ETA: 0s - loss: 0.3185 - acc: 0.896 - ETA: 0s - loss: 0.3075 - acc: 0.900 - ETA: 0s - loss: 0.3022 - acc: 0.903 - 5s 166ms/step - loss: 0.2927 - acc: 0.9067 - val_loss: 2.6313 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83333\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2018 - acc: 1.000 - ETA: 4s - loss: 0.1123 - acc: 1.000 - ETA: 4s - loss: 0.1143 - acc: 1.000 - ETA: 4s - loss: 0.1166 - acc: 1.000 - ETA: 3s - loss: 0.0965 - acc: 1.000 - ETA: 3s - loss: 0.0840 - acc: 1.000 - ETA: 3s - loss: 0.0729 - acc: 1.000 - ETA: 3s - loss: 0.0872 - acc: 0.975 - ETA: 3s - loss: 0.1486 - acc: 0.933 - ETA: 3s - loss: 0.1396 - acc: 0.940 - ETA: 2s - loss: 0.1303 - acc: 0.945 - ETA: 2s - loss: 0.1611 - acc: 0.933 - ETA: 2s - loss: 0.1497 - acc: 0.938 - ETA: 2s - loss: 0.1393 - acc: 0.942 - ETA: 2s - loss: 0.1353 - acc: 0.946 - ETA: 2s - loss: 0.1288 - acc: 0.950 - ETA: 2s - loss: 0.1233 - acc: 0.952 - ETA: 1s - loss: 0.1167 - acc: 0.955 - ETA: 1s - loss: 0.1127 - acc: 0.957 - ETA: 1s - loss: 0.1814 - acc: 0.920 - ETA: 1s - loss: 0.1904 - acc: 0.914 - ETA: 1s - loss: 0.1886 - acc: 0.918 - ETA: 1s - loss: 0.1805 - acc: 0.921 - ETA: 0s - loss: 0.1920 - acc: 0.908 - ETA: 0s - loss: 0.1868 - acc: 0.912 - ETA: 0s - loss: 0.1938 - acc: 0.907 - ETA: 0s - loss: 0.1891 - acc: 0.911 - ETA: 0s - loss: 0.1911 - acc: 0.914 - ETA: 0s - loss: 0.1849 - acc: 0.917 - 5s 165ms/step - loss: 0.1800 - acc: 0.9200 - val_loss: 2.9185 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83333\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0132 - acc: 1.000 - ETA: 4s - loss: 0.0286 - acc: 1.000 - ETA: 4s - loss: 0.0208 - acc: 1.000 - ETA: 4s - loss: 0.0222 - acc: 1.000 - ETA: 3s - loss: 0.0544 - acc: 1.000 - ETA: 3s - loss: 0.0505 - acc: 1.000 - ETA: 3s - loss: 0.0692 - acc: 0.971 - ETA: 3s - loss: 0.0667 - acc: 0.975 - ETA: 3s - loss: 0.0669 - acc: 0.977 - ETA: 3s - loss: 0.0650 - acc: 0.980 - ETA: 2s - loss: 0.0648 - acc: 0.981 - ETA: 2s - loss: 0.0614 - acc: 0.983 - ETA: 2s - loss: 0.0582 - acc: 0.984 - ETA: 2s - loss: 0.0615 - acc: 0.985 - ETA: 2s - loss: 0.0713 - acc: 0.986 - ETA: 2s - loss: 0.1116 - acc: 0.962 - ETA: 2s - loss: 0.1051 - acc: 0.964 - ETA: 1s - loss: 0.0998 - acc: 0.966 - ETA: 1s - loss: 0.0958 - acc: 0.968 - ETA: 1s - loss: 0.0924 - acc: 0.970 - ETA: 1s - loss: 0.0937 - acc: 0.971 - ETA: 1s - loss: 0.0898 - acc: 0.972 - ETA: 1s - loss: 0.1165 - acc: 0.956 - ETA: 0s - loss: 0.1298 - acc: 0.950 - ETA: 0s - loss: 0.1350 - acc: 0.944 - ETA: 0s - loss: 0.1337 - acc: 0.946 - ETA: 0s - loss: 0.1422 - acc: 0.940 - ETA: 0s - loss: 0.1450 - acc: 0.935 - ETA: 0s - loss: 0.1442 - acc: 0.937 - 5s 164ms/step - loss: 0.1409 - acc: 0.9400 - val_loss: 1.1472 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83333\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.4615 - acc: 0.800 - ETA: 4s - loss: 0.2332 - acc: 0.900 - ETA: 4s - loss: 0.1633 - acc: 0.933 - ETA: 4s - loss: 0.1313 - acc: 0.950 - ETA: 3s - loss: 0.1152 - acc: 0.960 - ETA: 3s - loss: 0.0963 - acc: 0.966 - ETA: 3s - loss: 0.1040 - acc: 0.942 - ETA: 3s - loss: 0.0917 - acc: 0.950 - ETA: 3s - loss: 0.0872 - acc: 0.955 - ETA: 3s - loss: 0.2099 - acc: 0.920 - ETA: 2s - loss: 0.1927 - acc: 0.927 - ETA: 2s - loss: 0.1864 - acc: 0.933 - ETA: 2s - loss: 0.1735 - acc: 0.938 - ETA: 2s - loss: 0.1614 - acc: 0.942 - ETA: 2s - loss: 0.1547 - acc: 0.946 - ETA: 2s - loss: 0.1499 - acc: 0.950 - ETA: 2s - loss: 0.1554 - acc: 0.952 - ETA: 1s - loss: 0.1469 - acc: 0.955 - ETA: 1s - loss: 0.1392 - acc: 0.957 - ETA: 1s - loss: 0.1471 - acc: 0.950 - ETA: 1s - loss: 0.1470 - acc: 0.952 - ETA: 1s - loss: 0.1405 - acc: 0.954 - ETA: 1s - loss: 0.1685 - acc: 0.939 - ETA: 0s - loss: 0.1635 - acc: 0.941 - ETA: 0s - loss: 0.1571 - acc: 0.944 - ETA: 0s - loss: 0.1542 - acc: 0.946 - ETA: 0s - loss: 0.2121 - acc: 0.933 - ETA: 0s - loss: 0.2378 - acc: 0.928 - ETA: 0s - loss: 0.2347 - acc: 0.931 - 5s 165ms/step - loss: 0.2274 - acc: 0.9333 - val_loss: 1.2967 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83333\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.0159 - acc: 0.800 - ETA: 4s - loss: 0.6795 - acc: 0.800 - ETA: 4s - loss: 0.6748 - acc: 0.800 - ETA: 4s - loss: 0.5207 - acc: 0.850 - ETA: 3s - loss: 1.0332 - acc: 0.760 - ETA: 3s - loss: 0.8804 - acc: 0.800 - ETA: 3s - loss: 0.7645 - acc: 0.828 - ETA: 3s - loss: 0.6692 - acc: 0.850 - ETA: 3s - loss: 0.5975 - acc: 0.866 - ETA: 3s - loss: 0.6984 - acc: 0.860 - ETA: 2s - loss: 0.6572 - acc: 0.872 - ETA: 2s - loss: 0.6962 - acc: 0.850 - ETA: 2s - loss: 0.6568 - acc: 0.861 - ETA: 2s - loss: 0.6465 - acc: 0.857 - ETA: 2s - loss: 0.6109 - acc: 0.866 - ETA: 2s - loss: 0.5801 - acc: 0.875 - ETA: 2s - loss: 0.5475 - acc: 0.882 - ETA: 1s - loss: 0.5261 - acc: 0.888 - ETA: 1s - loss: 0.5314 - acc: 0.884 - ETA: 1s - loss: 0.5425 - acc: 0.860 - ETA: 1s - loss: 0.5599 - acc: 0.847 - ETA: 1s - loss: 0.5452 - acc: 0.845 - ETA: 1s - loss: 0.5381 - acc: 0.843 - ETA: 0s - loss: 0.5237 - acc: 0.841 - ETA: 0s - loss: 0.5108 - acc: 0.840 - ETA: 0s - loss: 0.4955 - acc: 0.846 - ETA: 0s - loss: 0.4797 - acc: 0.851 - ETA: 0s - loss: 0.4707 - acc: 0.850 - ETA: 0s - loss: 0.5091 - acc: 0.841 - 5s 165ms/step - loss: 0.5130 - acc: 0.8333 - val_loss: 1.8551 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83333\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1123 - acc: 1.000 - ETA: 4s - loss: 0.4416 - acc: 0.900 - ETA: 4s - loss: 0.3008 - acc: 0.933 - ETA: 4s - loss: 0.2581 - acc: 0.950 - ETA: 3s - loss: 0.2176 - acc: 0.960 - ETA: 3s - loss: 0.1826 - acc: 0.966 - ETA: 3s - loss: 0.3264 - acc: 0.942 - ETA: 3s - loss: 0.2866 - acc: 0.950 - ETA: 3s - loss: 0.2789 - acc: 0.933 - ETA: 3s - loss: 0.2694 - acc: 0.940 - ETA: 2s - loss: 0.2695 - acc: 0.945 - ETA: 2s - loss: 0.2478 - acc: 0.950 - ETA: 2s - loss: 0.2853 - acc: 0.938 - ETA: 2s - loss: 0.2686 - acc: 0.942 - ETA: 2s - loss: 0.2541 - acc: 0.946 - ETA: 2s - loss: 0.2438 - acc: 0.950 - ETA: 2s - loss: 0.2346 - acc: 0.952 - ETA: 1s - loss: 0.2370 - acc: 0.944 - ETA: 1s - loss: 0.2472 - acc: 0.936 - ETA: 1s - loss: 0.2380 - acc: 0.940 - ETA: 1s - loss: 0.2319 - acc: 0.942 - ETA: 1s - loss: 0.2241 - acc: 0.945 - ETA: 1s - loss: 0.2148 - acc: 0.947 - ETA: 0s - loss: 0.2130 - acc: 0.950 - ETA: 0s - loss: 0.2114 - acc: 0.952 - ETA: 0s - loss: 0.2073 - acc: 0.953 - ETA: 0s - loss: 0.2394 - acc: 0.948 - ETA: 0s - loss: 0.2448 - acc: 0.942 - ETA: 0s - loss: 0.2371 - acc: 0.944 - 5s 165ms/step - loss: 0.2317 - acc: 0.9467 - val_loss: 0.9284 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83333\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0106 - acc: 1.000 - ETA: 4s - loss: 0.0139 - acc: 1.000 - ETA: 4s - loss: 0.1605 - acc: 0.866 - ETA: 4s - loss: 0.1389 - acc: 0.900 - ETA: 3s - loss: 0.1172 - acc: 0.920 - ETA: 3s - loss: 0.1320 - acc: 0.900 - ETA: 3s - loss: 0.1374 - acc: 0.885 - ETA: 3s - loss: 0.2167 - acc: 0.875 - ETA: 3s - loss: 0.2423 - acc: 0.866 - ETA: 3s - loss: 0.2663 - acc: 0.860 - ETA: 3s - loss: 0.2522 - acc: 0.872 - ETA: 2s - loss: 0.2369 - acc: 0.883 - ETA: 2s - loss: 0.2210 - acc: 0.892 - ETA: 2s - loss: 0.2546 - acc: 0.885 - ETA: 2s - loss: 0.2409 - acc: 0.893 - ETA: 2s - loss: 0.2260 - acc: 0.900 - ETA: 2s - loss: 0.2224 - acc: 0.905 - ETA: 1s - loss: 0.2140 - acc: 0.911 - ETA: 1s - loss: 0.2289 - acc: 0.905 - ETA: 1s - loss: 0.2447 - acc: 0.900 - ETA: 1s - loss: 0.2370 - acc: 0.904 - ETA: 1s - loss: 0.2264 - acc: 0.909 - ETA: 1s - loss: 0.2169 - acc: 0.913 - ETA: 0s - loss: 0.2921 - acc: 0.900 - ETA: 0s - loss: 0.3286 - acc: 0.888 - ETA: 0s - loss: 0.3164 - acc: 0.892 - ETA: 0s - loss: 0.3318 - acc: 0.881 - ETA: 0s - loss: 0.3252 - acc: 0.885 - ETA: 0s - loss: 0.3206 - acc: 0.882 - 5s 166ms/step - loss: 0.3141 - acc: 0.8867 - val_loss: 1.8842 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83333\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.0411 - acc: 0.600 - ETA: 4s - loss: 0.8854 - acc: 0.700 - ETA: 4s - loss: 0.5940 - acc: 0.800 - ETA: 4s - loss: 0.4528 - acc: 0.850 - ETA: 3s - loss: 0.3873 - acc: 0.880 - ETA: 3s - loss: 0.3506 - acc: 0.866 - ETA: 3s - loss: 0.4536 - acc: 0.857 - ETA: 3s - loss: 0.4351 - acc: 0.825 - ETA: 3s - loss: 0.3983 - acc: 0.844 - ETA: 3s - loss: 0.3613 - acc: 0.860 - ETA: 3s - loss: 0.3331 - acc: 0.872 - ETA: 2s - loss: 0.3444 - acc: 0.850 - ETA: 2s - loss: 0.3192 - acc: 0.861 - ETA: 2s - loss: 0.3526 - acc: 0.857 - ETA: 2s - loss: 0.3320 - acc: 0.866 - ETA: 2s - loss: 0.3175 - acc: 0.875 - ETA: 2s - loss: 0.3006 - acc: 0.882 - ETA: 1s - loss: 0.3316 - acc: 0.877 - ETA: 1s - loss: 0.3195 - acc: 0.884 - ETA: 1s - loss: 0.3082 - acc: 0.890 - ETA: 1s - loss: 0.2966 - acc: 0.895 - ETA: 1s - loss: 0.3561 - acc: 0.872 - ETA: 1s - loss: 0.3541 - acc: 0.869 - ETA: 0s - loss: 0.3585 - acc: 0.858 - ETA: 0s - loss: 0.3458 - acc: 0.864 - ETA: 0s - loss: 0.3573 - acc: 0.861 - ETA: 0s - loss: 0.3553 - acc: 0.859 - ETA: 0s - loss: 0.3570 - acc: 0.857 - ETA: 0s - loss: 0.3450 - acc: 0.862 - 5s 165ms/step - loss: 0.3347 - acc: 0.8667 - val_loss: 6.3808 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83333\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0092 - acc: 1.000 - ETA: 4s - loss: 0.0302 - acc: 1.000 - ETA: 4s - loss: 0.1774 - acc: 0.933 - ETA: 4s - loss: 0.2621 - acc: 0.900 - ETA: 3s - loss: 0.2164 - acc: 0.920 - ETA: 3s - loss: 0.2063 - acc: 0.933 - ETA: 3s - loss: 0.2508 - acc: 0.914 - ETA: 3s - loss: 0.2608 - acc: 0.900 - ETA: 3s - loss: 0.2593 - acc: 0.888 - ETA: 3s - loss: 0.2441 - acc: 0.900 - ETA: 3s - loss: 0.2270 - acc: 0.909 - ETA: 2s - loss: 0.4729 - acc: 0.883 - ETA: 2s - loss: 0.4379 - acc: 0.892 - ETA: 2s - loss: 0.4222 - acc: 0.885 - ETA: 2s - loss: 0.3952 - acc: 0.893 - ETA: 2s - loss: 0.4827 - acc: 0.887 - ETA: 2s - loss: 0.5355 - acc: 0.882 - ETA: 1s - loss: 0.6499 - acc: 0.855 - ETA: 1s - loss: 0.6157 - acc: 0.863 - ETA: 1s - loss: 0.6125 - acc: 0.860 - ETA: 1s - loss: 0.6510 - acc: 0.838 - ETA: 1s - loss: 0.6339 - acc: 0.836 - ETA: 1s - loss: 0.6275 - acc: 0.834 - ETA: 0s - loss: 0.6496 - acc: 0.825 - ETA: 0s - loss: 0.6453 - acc: 0.824 - ETA: 0s - loss: 0.6423 - acc: 0.815 - ETA: 0s - loss: 0.6355 - acc: 0.814 - ETA: 0s - loss: 0.6297 - acc: 0.814 - ETA: 0s - loss: 0.6182 - acc: 0.813 - 5s 165ms/step - loss: 0.6085 - acc: 0.8133 - val_loss: 1.0963 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83333\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.2979 - acc: 0.800 - ETA: 4s - loss: 0.2216 - acc: 0.900 - ETA: 4s - loss: 0.2980 - acc: 0.800 - ETA: 4s - loss: 0.2540 - acc: 0.850 - ETA: 4s - loss: 0.3072 - acc: 0.840 - ETA: 3s - loss: 0.2581 - acc: 0.866 - ETA: 3s - loss: 0.2218 - acc: 0.885 - ETA: 3s - loss: 0.2459 - acc: 0.850 - ETA: 3s - loss: 0.2298 - acc: 0.866 - ETA: 3s - loss: 0.2279 - acc: 0.860 - ETA: 3s - loss: 0.2835 - acc: 0.836 - ETA: 2s - loss: 0.2599 - acc: 0.850 - ETA: 2s - loss: 0.2420 - acc: 0.861 - ETA: 2s - loss: 0.2395 - acc: 0.857 - ETA: 2s - loss: 0.2742 - acc: 0.840 - ETA: 2s - loss: 0.2942 - acc: 0.825 - ETA: 2s - loss: 0.3358 - acc: 0.823 - ETA: 1s - loss: 0.3172 - acc: 0.833 - ETA: 1s - loss: 0.3251 - acc: 0.831 - ETA: 1s - loss: 0.3566 - acc: 0.830 - ETA: 1s - loss: 0.3598 - acc: 0.828 - ETA: 1s - loss: 0.3458 - acc: 0.836 - ETA: 1s - loss: 0.3433 - acc: 0.834 - ETA: 0s - loss: 0.3423 - acc: 0.833 - ETA: 0s - loss: 0.3449 - acc: 0.832 - ETA: 0s - loss: 0.3338 - acc: 0.838 - ETA: 0s - loss: 0.3239 - acc: 0.844 - ETA: 0s - loss: 0.3124 - acc: 0.850 - ETA: 0s - loss: 0.3046 - acc: 0.855 - 5s 166ms/step - loss: 0.3054 - acc: 0.8533 - val_loss: 6.9846 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83333\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1354 - acc: 1.000 - ETA: 4s - loss: 0.0793 - acc: 1.000 - ETA: 4s - loss: 0.1671 - acc: 0.933 - ETA: 4s - loss: 0.1475 - acc: 0.950 - ETA: 3s - loss: 0.1977 - acc: 0.920 - ETA: 3s - loss: 0.1776 - acc: 0.933 - ETA: 3s - loss: 0.3808 - acc: 0.914 - ETA: 3s - loss: 0.3420 - acc: 0.925 - ETA: 3s - loss: 0.3296 - acc: 0.911 - ETA: 3s - loss: 0.3535 - acc: 0.880 - ETA: 3s - loss: 0.3760 - acc: 0.872 - ETA: 2s - loss: 0.3472 - acc: 0.883 - ETA: 2s - loss: 0.3481 - acc: 0.861 - ETA: 2s - loss: 0.3584 - acc: 0.857 - ETA: 2s - loss: 0.3358 - acc: 0.866 - ETA: 2s - loss: 0.3254 - acc: 0.862 - ETA: 2s - loss: 0.3371 - acc: 0.858 - ETA: 1s - loss: 0.3189 - acc: 0.866 - ETA: 1s - loss: 0.3049 - acc: 0.873 - ETA: 1s - loss: 0.2933 - acc: 0.880 - ETA: 1s - loss: 0.2815 - acc: 0.885 - ETA: 1s - loss: 0.2694 - acc: 0.890 - ETA: 1s - loss: 0.2585 - acc: 0.895 - ETA: 0s - loss: 0.2725 - acc: 0.883 - ETA: 0s - loss: 0.2628 - acc: 0.888 - ETA: 0s - loss: 0.2593 - acc: 0.884 - ETA: 0s - loss: 0.2669 - acc: 0.881 - ETA: 0s - loss: 0.2599 - acc: 0.885 - ETA: 0s - loss: 0.2609 - acc: 0.882 - 5s 165ms/step - loss: 0.2535 - acc: 0.8867 - val_loss: 6.2263 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83333\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0605 - acc: 1.000 - ETA: 4s - loss: 0.2318 - acc: 0.900 - ETA: 4s - loss: 0.1974 - acc: 0.933 - ETA: 4s - loss: 0.1518 - acc: 0.950 - ETA: 4s - loss: 0.1311 - acc: 0.960 - ETA: 3s - loss: 0.1638 - acc: 0.933 - ETA: 3s - loss: 0.2348 - acc: 0.885 - ETA: 3s - loss: 0.2060 - acc: 0.900 - ETA: 3s - loss: 0.1995 - acc: 0.911 - ETA: 3s - loss: 0.1833 - acc: 0.920 - ETA: 3s - loss: 0.1711 - acc: 0.927 - ETA: 2s - loss: 0.1578 - acc: 0.933 - ETA: 2s - loss: 0.1802 - acc: 0.907 - ETA: 2s - loss: 0.1727 - acc: 0.914 - ETA: 2s - loss: 0.1634 - acc: 0.920 - ETA: 2s - loss: 0.1599 - acc: 0.925 - ETA: 2s - loss: 0.1540 - acc: 0.929 - ETA: 1s - loss: 0.1534 - acc: 0.933 - ETA: 1s - loss: 0.1742 - acc: 0.915 - ETA: 1s - loss: 0.1669 - acc: 0.920 - ETA: 1s - loss: 0.1637 - acc: 0.923 - ETA: 1s - loss: 0.1720 - acc: 0.918 - ETA: 1s - loss: 0.1705 - acc: 0.921 - ETA: 0s - loss: 0.1685 - acc: 0.925 - ETA: 0s - loss: 0.1641 - acc: 0.928 - ETA: 0s - loss: 0.1596 - acc: 0.930 - ETA: 0s - loss: 0.1674 - acc: 0.925 - ETA: 0s - loss: 0.1635 - acc: 0.928 - ETA: 0s - loss: 0.1586 - acc: 0.931 - 5s 166ms/step - loss: 0.1544 - acc: 0.9333 - val_loss: 1.3708 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83333\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0256 - acc: 1.000 - ETA: 4s - loss: 0.0258 - acc: 1.000 - ETA: 4s - loss: 0.0262 - acc: 1.000 - ETA: 4s - loss: 0.0643 - acc: 0.950 - ETA: 3s - loss: 0.1076 - acc: 0.920 - ETA: 3s - loss: 0.0903 - acc: 0.933 - ETA: 3s - loss: 0.0828 - acc: 0.942 - ETA: 3s - loss: 0.0731 - acc: 0.950 - ETA: 3s - loss: 0.0808 - acc: 0.955 - ETA: 3s - loss: 0.0865 - acc: 0.960 - ETA: 3s - loss: 0.0847 - acc: 0.963 - ETA: 2s - loss: 0.0781 - acc: 0.966 - ETA: 2s - loss: 0.0733 - acc: 0.969 - ETA: 2s - loss: 0.0717 - acc: 0.971 - ETA: 2s - loss: 0.0741 - acc: 0.973 - ETA: 2s - loss: 0.0702 - acc: 0.975 - ETA: 2s - loss: 0.0673 - acc: 0.976 - ETA: 1s - loss: 0.0638 - acc: 0.977 - ETA: 1s - loss: 0.0620 - acc: 0.978 - ETA: 1s - loss: 0.0591 - acc: 0.980 - ETA: 1s - loss: 0.0627 - acc: 0.981 - ETA: 1s - loss: 0.0640 - acc: 0.981 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 0s - loss: 0.0594 - acc: 0.983 - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0567 - acc: 0.984 - ETA: 0s - loss: 0.0891 - acc: 0.977 - ETA: 0s - loss: 0.1091 - acc: 0.971 - ETA: 0s - loss: 0.1059 - acc: 0.972 - 5s 165ms/step - loss: 0.1032 - acc: 0.9733 - val_loss: 0.8764 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83333\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0145 - acc: 1.000 - ETA: 4s - loss: 0.0344 - acc: 1.000 - ETA: 4s - loss: 0.0264 - acc: 1.000 - ETA: 4s - loss: 0.0276 - acc: 1.000 - ETA: 3s - loss: 0.0232 - acc: 1.000 - ETA: 3s - loss: 0.0208 - acc: 1.000 - ETA: 3s - loss: 0.0370 - acc: 1.000 - ETA: 3s - loss: 0.1259 - acc: 0.975 - ETA: 3s - loss: 0.1651 - acc: 0.933 - ETA: 3s - loss: 0.1490 - acc: 0.940 - ETA: 3s - loss: 0.1355 - acc: 0.945 - ETA: 2s - loss: 0.1251 - acc: 0.950 - ETA: 2s - loss: 0.1665 - acc: 0.923 - ETA: 2s - loss: 0.1682 - acc: 0.914 - ETA: 2s - loss: 0.1716 - acc: 0.906 - ETA: 2s - loss: 0.1616 - acc: 0.912 - ETA: 2s - loss: 0.1667 - acc: 0.905 - ETA: 1s - loss: 0.1689 - acc: 0.911 - ETA: 1s - loss: 0.1630 - acc: 0.915 - ETA: 1s - loss: 0.1578 - acc: 0.920 - ETA: 1s - loss: 0.1850 - acc: 0.914 - ETA: 1s - loss: 0.1773 - acc: 0.918 - ETA: 1s - loss: 0.1743 - acc: 0.921 - ETA: 0s - loss: 0.1678 - acc: 0.925 - ETA: 0s - loss: 0.1655 - acc: 0.928 - ETA: 0s - loss: 0.1603 - acc: 0.930 - ETA: 0s - loss: 0.1577 - acc: 0.933 - ETA: 0s - loss: 0.1628 - acc: 0.928 - ETA: 0s - loss: 0.1588 - acc: 0.931 - 5s 165ms/step - loss: 0.1565 - acc: 0.9333 - val_loss: 0.6751 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83333\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0166 - acc: 1.000 - ETA: 4s - loss: 0.0422 - acc: 1.000 - ETA: 4s - loss: 0.0512 - acc: 1.000 - ETA: 4s - loss: 0.0469 - acc: 1.000 - ETA: 3s - loss: 0.0385 - acc: 1.000 - ETA: 3s - loss: 0.0334 - acc: 1.000 - ETA: 3s - loss: 0.0377 - acc: 1.000 - ETA: 3s - loss: 0.0341 - acc: 1.000 - ETA: 3s - loss: 0.0856 - acc: 0.977 - ETA: 3s - loss: 0.0819 - acc: 0.980 - ETA: 2s - loss: 0.0748 - acc: 0.981 - ETA: 2s - loss: 0.0700 - acc: 0.983 - ETA: 2s - loss: 0.0676 - acc: 0.984 - ETA: 2s - loss: 0.0840 - acc: 0.971 - ETA: 2s - loss: 0.0789 - acc: 0.973 - ETA: 2s - loss: 0.1064 - acc: 0.950 - ETA: 2s - loss: 0.1142 - acc: 0.941 - ETA: 1s - loss: 0.1084 - acc: 0.944 - ETA: 1s - loss: 0.1261 - acc: 0.936 - ETA: 1s - loss: 0.1674 - acc: 0.930 - ETA: 1s - loss: 0.1608 - acc: 0.933 - ETA: 1s - loss: 0.1626 - acc: 0.927 - ETA: 1s - loss: 0.1568 - acc: 0.930 - ETA: 0s - loss: 0.1513 - acc: 0.933 - ETA: 0s - loss: 0.1513 - acc: 0.936 - ETA: 0s - loss: 0.1634 - acc: 0.930 - ETA: 0s - loss: 0.1576 - acc: 0.933 - ETA: 0s - loss: 0.1522 - acc: 0.935 - ETA: 0s - loss: 0.1487 - acc: 0.937 - 5s 165ms/step - loss: 0.1455 - acc: 0.9400 - val_loss: 1.0522 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83333\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0348 - acc: 1.000 - ETA: 4s - loss: 0.0403 - acc: 1.000 - ETA: 4s - loss: 0.0405 - acc: 1.000 - ETA: 4s - loss: 0.0368 - acc: 1.000 - ETA: 4s - loss: 0.0347 - acc: 1.000 - ETA: 3s - loss: 0.0542 - acc: 1.000 - ETA: 3s - loss: 0.0490 - acc: 1.000 - ETA: 3s - loss: 0.0468 - acc: 1.000 - ETA: 3s - loss: 0.0565 - acc: 1.000 - ETA: 3s - loss: 0.0529 - acc: 1.000 - ETA: 3s - loss: 0.0489 - acc: 1.000 - ETA: 2s - loss: 0.0453 - acc: 1.000 - ETA: 2s - loss: 0.0484 - acc: 1.000 - ETA: 2s - loss: 0.0573 - acc: 1.000 - ETA: 2s - loss: 0.0541 - acc: 1.000 - ETA: 2s - loss: 0.0552 - acc: 1.000 - ETA: 2s - loss: 0.0581 - acc: 1.000 - ETA: 1s - loss: 0.1078 - acc: 0.977 - ETA: 1s - loss: 0.1030 - acc: 0.978 - ETA: 1s - loss: 0.0987 - acc: 0.980 - ETA: 1s - loss: 0.0951 - acc: 0.981 - ETA: 1s - loss: 0.0918 - acc: 0.981 - ETA: 1s - loss: 0.0881 - acc: 0.982 - ETA: 0s - loss: 0.0854 - acc: 0.983 - ETA: 0s - loss: 0.0829 - acc: 0.984 - ETA: 0s - loss: 0.0805 - acc: 0.984 - ETA: 0s - loss: 0.0795 - acc: 0.985 - ETA: 0s - loss: 0.0783 - acc: 0.985 - ETA: 0s - loss: 0.0763 - acc: 0.986 - 5s 166ms/step - loss: 0.0738 - acc: 0.9867 - val_loss: 2.9248 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83333\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.4140 - acc: 0.800 - ETA: 4s - loss: 0.2075 - acc: 0.900 - ETA: 4s - loss: 0.1443 - acc: 0.933 - ETA: 4s - loss: 0.3090 - acc: 0.900 - ETA: 3s - loss: 0.2477 - acc: 0.920 - ETA: 3s - loss: 0.3121 - acc: 0.866 - ETA: 3s - loss: 0.2687 - acc: 0.885 - ETA: 3s - loss: 0.2438 - acc: 0.900 - ETA: 3s - loss: 0.2827 - acc: 0.866 - ETA: 3s - loss: 0.2658 - acc: 0.880 - ETA: 3s - loss: 0.2482 - acc: 0.890 - ETA: 2s - loss: 0.2294 - acc: 0.900 - ETA: 2s - loss: 0.2150 - acc: 0.907 - ETA: 2s - loss: 0.2145 - acc: 0.914 - ETA: 2s - loss: 0.2395 - acc: 0.906 - ETA: 2s - loss: 0.2338 - acc: 0.900 - ETA: 2s - loss: 0.2624 - acc: 0.894 - ETA: 1s - loss: 0.2540 - acc: 0.900 - ETA: 1s - loss: 0.2407 - acc: 0.905 - ETA: 1s - loss: 0.2294 - acc: 0.910 - ETA: 1s - loss: 0.2364 - acc: 0.904 - ETA: 1s - loss: 0.2326 - acc: 0.909 - ETA: 1s - loss: 0.2248 - acc: 0.913 - ETA: 0s - loss: 0.2161 - acc: 0.916 - ETA: 0s - loss: 0.2085 - acc: 0.920 - ETA: 0s - loss: 0.2050 - acc: 0.923 - ETA: 0s - loss: 0.1979 - acc: 0.925 - ETA: 0s - loss: 0.1940 - acc: 0.928 - ETA: 0s - loss: 0.1876 - acc: 0.931 - 5s 166ms/step - loss: 0.1850 - acc: 0.9333 - val_loss: 0.7144 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83333\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0284 - acc: 1.000 - ETA: 4s - loss: 0.0266 - acc: 1.000 - ETA: 4s - loss: 0.0326 - acc: 1.000 - ETA: 4s - loss: 0.0428 - acc: 1.000 - ETA: 4s - loss: 0.0526 - acc: 1.000 - ETA: 3s - loss: 0.0448 - acc: 1.000 - ETA: 3s - loss: 0.0396 - acc: 1.000 - ETA: 3s - loss: 0.0399 - acc: 1.000 - ETA: 3s - loss: 0.0360 - acc: 1.000 - ETA: 3s - loss: 0.0354 - acc: 1.000 - ETA: 3s - loss: 0.0339 - acc: 1.000 - ETA: 2s - loss: 0.0314 - acc: 1.000 - ETA: 2s - loss: 0.0337 - acc: 1.000 - ETA: 2s - loss: 0.0345 - acc: 1.000 - ETA: 2s - loss: 0.0330 - acc: 1.000 - ETA: 2s - loss: 0.0312 - acc: 1.000 - ETA: 2s - loss: 0.0297 - acc: 1.000 - ETA: 1s - loss: 0.0292 - acc: 1.000 - ETA: 1s - loss: 0.0278 - acc: 1.000 - ETA: 1s - loss: 0.0267 - acc: 1.000 - ETA: 1s - loss: 0.0255 - acc: 1.000 - ETA: 1s - loss: 0.0245 - acc: 1.000 - ETA: 1s - loss: 0.0237 - acc: 1.000 - ETA: 0s - loss: 0.0252 - acc: 1.000 - ETA: 0s - loss: 0.0244 - acc: 1.000 - ETA: 0s - loss: 0.0235 - acc: 1.000 - ETA: 0s - loss: 0.0264 - acc: 1.000 - ETA: 0s - loss: 0.0259 - acc: 1.000 - ETA: 0s - loss: 0.0258 - acc: 1.000 - 5s 166ms/step - loss: 0.0250 - acc: 1.0000 - val_loss: 0.9087 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83333\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2258 - acc: 0.800 - ETA: 4s - loss: 0.1352 - acc: 0.900 - ETA: 4s - loss: 0.0906 - acc: 0.933 - ETA: 4s - loss: 0.0718 - acc: 0.950 - ETA: 3s - loss: 0.2031 - acc: 0.880 - ETA: 3s - loss: 0.1820 - acc: 0.900 - ETA: 3s - loss: 0.1692 - acc: 0.914 - ETA: 3s - loss: 0.1499 - acc: 0.925 - ETA: 3s - loss: 0.1336 - acc: 0.933 - ETA: 3s - loss: 0.1261 - acc: 0.940 - ETA: 3s - loss: 0.1510 - acc: 0.927 - ETA: 2s - loss: 0.1450 - acc: 0.933 - ETA: 2s - loss: 0.1353 - acc: 0.938 - ETA: 2s - loss: 0.1561 - acc: 0.928 - ETA: 2s - loss: 0.1478 - acc: 0.933 - ETA: 2s - loss: 0.1389 - acc: 0.937 - ETA: 2s - loss: 0.1380 - acc: 0.941 - ETA: 1s - loss: 0.1317 - acc: 0.944 - ETA: 1s - loss: 0.1305 - acc: 0.947 - ETA: 1s - loss: 0.1279 - acc: 0.950 - ETA: 1s - loss: 0.1219 - acc: 0.952 - ETA: 1s - loss: 0.1178 - acc: 0.954 - ETA: 1s - loss: 0.1171 - acc: 0.956 - ETA: 0s - loss: 0.1123 - acc: 0.958 - ETA: 0s - loss: 0.1107 - acc: 0.960 - ETA: 0s - loss: 0.1069 - acc: 0.961 - ETA: 0s - loss: 0.1036 - acc: 0.963 - ETA: 0s - loss: 0.1245 - acc: 0.957 - ETA: 0s - loss: 0.1413 - acc: 0.951 - 5s 166ms/step - loss: 0.1367 - acc: 0.9533 - val_loss: 1.1938 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83333\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.3888 - acc: 0.800 - ETA: 4s - loss: 0.2156 - acc: 0.900 - ETA: 4s - loss: 0.1817 - acc: 0.933 - ETA: 4s - loss: 0.1365 - acc: 0.950 - ETA: 3s - loss: 0.1169 - acc: 0.960 - ETA: 3s - loss: 0.0981 - acc: 0.966 - ETA: 3s - loss: 0.0843 - acc: 0.971 - ETA: 3s - loss: 0.0882 - acc: 0.975 - ETA: 3s - loss: 0.0788 - acc: 0.977 - ETA: 3s - loss: 0.0898 - acc: 0.960 - ETA: 2s - loss: 0.1169 - acc: 0.945 - ETA: 2s - loss: 0.2031 - acc: 0.916 - ETA: 2s - loss: 0.1875 - acc: 0.923 - ETA: 2s - loss: 0.1759 - acc: 0.928 - ETA: 2s - loss: 0.2569 - acc: 0.920 - ETA: 2s - loss: 0.2410 - acc: 0.925 - ETA: 2s - loss: 0.2275 - acc: 0.929 - ETA: 1s - loss: 0.2158 - acc: 0.933 - ETA: 1s - loss: 0.2047 - acc: 0.936 - ETA: 1s - loss: 0.1994 - acc: 0.940 - ETA: 1s - loss: 0.1910 - acc: 0.942 - ETA: 1s - loss: 0.1826 - acc: 0.945 - ETA: 1s - loss: 0.1750 - acc: 0.947 - ETA: 0s - loss: 0.1678 - acc: 0.950 - ETA: 0s - loss: 0.1615 - acc: 0.952 - ETA: 0s - loss: 0.1658 - acc: 0.946 - ETA: 0s - loss: 0.1774 - acc: 0.940 - ETA: 0s - loss: 0.1714 - acc: 0.942 - ETA: 0s - loss: 0.1691 - acc: 0.944 - 5s 165ms/step - loss: 0.1644 - acc: 0.9467 - val_loss: 1.4694 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83333\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0576 - acc: 1.000 - ETA: 4s - loss: 0.0374 - acc: 1.000 - ETA: 4s - loss: 0.0423 - acc: 1.000 - ETA: 4s - loss: 0.1956 - acc: 0.950 - ETA: 3s - loss: 0.1605 - acc: 0.960 - ETA: 3s - loss: 0.1582 - acc: 0.966 - ETA: 3s - loss: 0.1442 - acc: 0.971 - ETA: 3s - loss: 0.1395 - acc: 0.975 - ETA: 3s - loss: 0.2311 - acc: 0.933 - ETA: 3s - loss: 0.2081 - acc: 0.940 - ETA: 2s - loss: 0.2070 - acc: 0.945 - ETA: 2s - loss: 0.1985 - acc: 0.950 - ETA: 2s - loss: 0.1851 - acc: 0.953 - ETA: 2s - loss: 0.1781 - acc: 0.957 - ETA: 2s - loss: 0.1670 - acc: 0.960 - ETA: 2s - loss: 0.1607 - acc: 0.962 - ETA: 2s - loss: 0.2062 - acc: 0.941 - ETA: 1s - loss: 0.1973 - acc: 0.944 - ETA: 1s - loss: 0.1941 - acc: 0.947 - ETA: 1s - loss: 0.2052 - acc: 0.940 - ETA: 1s - loss: 0.1981 - acc: 0.942 - ETA: 1s - loss: 0.2058 - acc: 0.936 - ETA: 1s - loss: 0.2007 - acc: 0.939 - ETA: 0s - loss: 0.1929 - acc: 0.941 - ETA: 0s - loss: 0.1870 - acc: 0.944 - ETA: 0s - loss: 0.1811 - acc: 0.946 - ETA: 0s - loss: 0.1757 - acc: 0.948 - ETA: 0s - loss: 0.1696 - acc: 0.950 - ETA: 0s - loss: 0.1647 - acc: 0.951 - 5s 164ms/step - loss: 0.1638 - acc: 0.9533 - val_loss: 4.3638 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83333\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.3512 - acc: 0.800 - ETA: 4s - loss: 0.1897 - acc: 0.900 - ETA: 4s - loss: 0.1640 - acc: 0.933 - ETA: 4s - loss: 0.1231 - acc: 0.950 - ETA: 4s - loss: 0.1246 - acc: 0.960 - ETA: 3s - loss: 0.1057 - acc: 0.966 - ETA: 3s - loss: 0.2040 - acc: 0.942 - ETA: 3s - loss: 0.1789 - acc: 0.950 - ETA: 3s - loss: 0.1617 - acc: 0.955 - ETA: 3s - loss: 0.1456 - acc: 0.960 - ETA: 3s - loss: 0.1412 - acc: 0.963 - ETA: 2s - loss: 0.1409 - acc: 0.966 - ETA: 2s - loss: 0.1451 - acc: 0.953 - ETA: 2s - loss: 0.3120 - acc: 0.914 - ETA: 2s - loss: 0.4082 - acc: 0.893 - ETA: 2s - loss: 0.3847 - acc: 0.900 - ETA: 2s - loss: 0.3621 - acc: 0.905 - ETA: 1s - loss: 0.3435 - acc: 0.911 - ETA: 1s - loss: 0.3623 - acc: 0.894 - ETA: 1s - loss: 0.3931 - acc: 0.880 - ETA: 1s - loss: 0.3824 - acc: 0.876 - ETA: 1s - loss: 0.3679 - acc: 0.881 - ETA: 1s - loss: 0.4321 - acc: 0.852 - ETA: 0s - loss: 0.4156 - acc: 0.858 - ETA: 0s - loss: 0.3992 - acc: 0.864 - ETA: 0s - loss: 0.3972 - acc: 0.861 - ETA: 0s - loss: 0.3892 - acc: 0.866 - ETA: 0s - loss: 0.3772 - acc: 0.871 - ETA: 0s - loss: 0.3788 - acc: 0.869 - 5s 166ms/step - loss: 0.3819 - acc: 0.8600 - val_loss: 2.1167 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83333\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0656 - acc: 1.000 - ETA: 4s - loss: 0.0888 - acc: 1.000 - ETA: 4s - loss: 0.0786 - acc: 1.000 - ETA: 4s - loss: 0.0725 - acc: 1.000 - ETA: 3s - loss: 0.1105 - acc: 0.960 - ETA: 3s - loss: 0.0945 - acc: 0.966 - ETA: 3s - loss: 0.0908 - acc: 0.971 - ETA: 3s - loss: 0.0800 - acc: 0.975 - ETA: 3s - loss: 0.3077 - acc: 0.933 - ETA: 3s - loss: 0.2779 - acc: 0.940 - ETA: 2s - loss: 0.2557 - acc: 0.945 - ETA: 2s - loss: 0.2400 - acc: 0.950 - ETA: 2s - loss: 0.2857 - acc: 0.938 - ETA: 2s - loss: 0.3556 - acc: 0.900 - ETA: 2s - loss: 0.3321 - acc: 0.906 - ETA: 2s - loss: 0.3231 - acc: 0.912 - ETA: 2s - loss: 0.3052 - acc: 0.917 - ETA: 1s - loss: 0.2976 - acc: 0.922 - ETA: 1s - loss: 0.2841 - acc: 0.926 - ETA: 1s - loss: 0.2772 - acc: 0.930 - ETA: 1s - loss: 0.2956 - acc: 0.923 - ETA: 1s - loss: 0.2925 - acc: 0.918 - ETA: 1s - loss: 0.2975 - acc: 0.913 - ETA: 0s - loss: 0.2865 - acc: 0.916 - ETA: 0s - loss: 0.2808 - acc: 0.920 - ETA: 0s - loss: 0.2744 - acc: 0.923 - ETA: 0s - loss: 0.2796 - acc: 0.918 - ETA: 0s - loss: 0.2710 - acc: 0.921 - ETA: 0s - loss: 0.2808 - acc: 0.910 - 5s 165ms/step - loss: 0.2855 - acc: 0.9067 - val_loss: 2.0058 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83333\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.6487 - acc: 0.800 - ETA: 4s - loss: 0.4595 - acc: 0.800 - ETA: 4s - loss: 0.3484 - acc: 0.866 - ETA: 4s - loss: 0.2659 - acc: 0.900 - ETA: 3s - loss: 0.2227 - acc: 0.920 - ETA: 3s - loss: 0.2956 - acc: 0.866 - ETA: 3s - loss: 0.3223 - acc: 0.828 - ETA: 3s - loss: 0.3018 - acc: 0.850 - ETA: 3s - loss: 0.2731 - acc: 0.866 - ETA: 3s - loss: 0.2510 - acc: 0.880 - ETA: 3s - loss: 0.2376 - acc: 0.890 - ETA: 2s - loss: 0.2197 - acc: 0.900 - ETA: 2s - loss: 0.2072 - acc: 0.907 - ETA: 2s - loss: 0.1928 - acc: 0.914 - ETA: 2s - loss: 0.1914 - acc: 0.906 - ETA: 2s - loss: 0.1825 - acc: 0.912 - ETA: 2s - loss: 0.2334 - acc: 0.905 - ETA: 1s - loss: 0.2348 - acc: 0.911 - ETA: 1s - loss: 0.2262 - acc: 0.915 - ETA: 1s - loss: 0.2156 - acc: 0.920 - ETA: 1s - loss: 0.2094 - acc: 0.923 - ETA: 1s - loss: 0.2021 - acc: 0.927 - ETA: 1s - loss: 0.1986 - acc: 0.930 - ETA: 0s - loss: 0.1908 - acc: 0.933 - ETA: 0s - loss: 0.1839 - acc: 0.936 - ETA: 0s - loss: 0.1771 - acc: 0.938 - ETA: 0s - loss: 0.1706 - acc: 0.940 - ETA: 0s - loss: 0.1650 - acc: 0.942 - ETA: 0s - loss: 0.1604 - acc: 0.944 - 5s 165ms/step - loss: 0.1578 - acc: 0.9467 - val_loss: 2.2077 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83333\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0732 - acc: 1.000 - ETA: 4s - loss: 0.0886 - acc: 1.000 - ETA: 4s - loss: 0.0783 - acc: 1.000 - ETA: 4s - loss: 0.0728 - acc: 1.000 - ETA: 3s - loss: 0.0731 - acc: 1.000 - ETA: 3s - loss: 0.0615 - acc: 1.000 - ETA: 3s - loss: 0.0758 - acc: 1.000 - ETA: 3s - loss: 0.0664 - acc: 1.000 - ETA: 3s - loss: 0.0600 - acc: 1.000 - ETA: 3s - loss: 0.0648 - acc: 1.000 - ETA: 3s - loss: 0.0627 - acc: 1.000 - ETA: 2s - loss: 0.0696 - acc: 0.983 - ETA: 2s - loss: 0.0658 - acc: 0.984 - ETA: 2s - loss: 0.0612 - acc: 0.985 - ETA: 2s - loss: 0.0572 - acc: 0.986 - ETA: 2s - loss: 0.0547 - acc: 0.987 - ETA: 2s - loss: 0.0518 - acc: 0.988 - ETA: 1s - loss: 0.0491 - acc: 0.988 - ETA: 1s - loss: 0.0546 - acc: 0.978 - ETA: 1s - loss: 0.0530 - acc: 0.980 - ETA: 1s - loss: 0.0507 - acc: 0.981 - ETA: 1s - loss: 0.0485 - acc: 0.981 - ETA: 1s - loss: 0.0464 - acc: 0.982 - ETA: 0s - loss: 0.0520 - acc: 0.975 - ETA: 0s - loss: 0.0501 - acc: 0.976 - ETA: 0s - loss: 0.0506 - acc: 0.976 - ETA: 0s - loss: 0.0489 - acc: 0.977 - ETA: 0s - loss: 0.0490 - acc: 0.978 - ETA: 0s - loss: 0.0474 - acc: 0.979 - 5s 166ms/step - loss: 0.0477 - acc: 0.9800 - val_loss: 1.2415 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83333\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0053 - acc: 1.000 - ETA: 4s - loss: 0.0541 - acc: 1.000 - ETA: 4s - loss: 0.0378 - acc: 1.000 - ETA: 4s - loss: 0.3124 - acc: 0.850 - ETA: 4s - loss: 0.2501 - acc: 0.880 - ETA: 3s - loss: 0.2268 - acc: 0.900 - ETA: 3s - loss: 0.1945 - acc: 0.914 - ETA: 3s - loss: 0.1713 - acc: 0.925 - ETA: 3s - loss: 0.1539 - acc: 0.933 - ETA: 3s - loss: 0.1618 - acc: 0.920 - ETA: 3s - loss: 0.1835 - acc: 0.909 - ETA: 2s - loss: 0.1712 - acc: 0.916 - ETA: 2s - loss: 0.1581 - acc: 0.923 - ETA: 2s - loss: 0.1470 - acc: 0.928 - ETA: 2s - loss: 0.1447 - acc: 0.933 - ETA: 2s - loss: 0.2208 - acc: 0.925 - ETA: 2s - loss: 0.2138 - acc: 0.929 - ETA: 1s - loss: 0.2043 - acc: 0.933 - ETA: 1s - loss: 0.1945 - acc: 0.936 - ETA: 1s - loss: 0.2147 - acc: 0.930 - ETA: 1s - loss: 0.2053 - acc: 0.933 - ETA: 1s - loss: 0.2012 - acc: 0.936 - ETA: 1s - loss: 0.2130 - acc: 0.930 - ETA: 0s - loss: 0.2057 - acc: 0.933 - ETA: 0s - loss: 0.2364 - acc: 0.920 - ETA: 0s - loss: 0.2284 - acc: 0.923 - ETA: 0s - loss: 0.2204 - acc: 0.925 - ETA: 0s - loss: 0.2206 - acc: 0.921 - ETA: 0s - loss: 0.2145 - acc: 0.924 - 5s 165ms/step - loss: 0.2092 - acc: 0.9267 - val_loss: 1.2931 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83333\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0701 - acc: 1.000 - ETA: 4s - loss: 0.0580 - acc: 1.000 - ETA: 4s - loss: 0.0722 - acc: 1.000 - ETA: 4s - loss: 0.0578 - acc: 1.000 - ETA: 3s - loss: 0.0600 - acc: 1.000 - ETA: 3s - loss: 0.0528 - acc: 1.000 - ETA: 3s - loss: 0.0653 - acc: 1.000 - ETA: 3s - loss: 0.0631 - acc: 1.000 - ETA: 3s - loss: 0.0564 - acc: 1.000 - ETA: 3s - loss: 0.0594 - acc: 1.000 - ETA: 3s - loss: 0.0546 - acc: 1.000 - ETA: 2s - loss: 0.0507 - acc: 1.000 - ETA: 2s - loss: 0.0500 - acc: 1.000 - ETA: 2s - loss: 0.0468 - acc: 1.000 - ETA: 2s - loss: 0.0462 - acc: 1.000 - ETA: 2s - loss: 0.0437 - acc: 1.000 - ETA: 2s - loss: 0.0425 - acc: 1.000 - ETA: 1s - loss: 0.0409 - acc: 1.000 - ETA: 1s - loss: 0.0409 - acc: 1.000 - ETA: 1s - loss: 0.0393 - acc: 1.000 - ETA: 1s - loss: 0.0397 - acc: 1.000 - ETA: 1s - loss: 0.0381 - acc: 1.000 - ETA: 1s - loss: 0.0380 - acc: 1.000 - ETA: 0s - loss: 0.0374 - acc: 1.000 - ETA: 0s - loss: 0.0360 - acc: 1.000 - ETA: 0s - loss: 0.0359 - acc: 1.000 - ETA: 0s - loss: 0.0364 - acc: 1.000 - ETA: 0s - loss: 0.0372 - acc: 1.000 - ETA: 0s - loss: 0.0373 - acc: 1.000 - 5s 166ms/step - loss: 0.0366 - acc: 1.0000 - val_loss: 1.0578 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83333\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0026 - acc: 1.000 - ETA: 4s - loss: 0.0294 - acc: 1.000 - ETA: 4s - loss: 0.0275 - acc: 1.000 - ETA: 4s - loss: 0.0283 - acc: 1.000 - ETA: 4s - loss: 0.0242 - acc: 1.000 - ETA: 3s - loss: 0.0207 - acc: 1.000 - ETA: 3s - loss: 0.0264 - acc: 1.000 - ETA: 3s - loss: 0.0561 - acc: 0.975 - ETA: 3s - loss: 0.0509 - acc: 0.977 - ETA: 3s - loss: 0.0492 - acc: 0.980 - ETA: 3s - loss: 0.0452 - acc: 0.981 - ETA: 2s - loss: 0.0417 - acc: 0.983 - ETA: 2s - loss: 0.0389 - acc: 0.984 - ETA: 2s - loss: 0.0512 - acc: 0.971 - ETA: 2s - loss: 0.0481 - acc: 0.973 - ETA: 2s - loss: 0.0452 - acc: 0.975 - ETA: 2s - loss: 0.0426 - acc: 0.976 - ETA: 1s - loss: 0.0408 - acc: 0.977 - ETA: 1s - loss: 0.0389 - acc: 0.978 - ETA: 1s - loss: 0.0471 - acc: 0.970 - ETA: 1s - loss: 0.0454 - acc: 0.971 - ETA: 1s - loss: 0.0450 - acc: 0.972 - ETA: 1s - loss: 0.0445 - acc: 0.973 - ETA: 0s - loss: 0.0432 - acc: 0.975 - ETA: 0s - loss: 0.0419 - acc: 0.976 - ETA: 0s - loss: 0.0409 - acc: 0.976 - ETA: 0s - loss: 0.0395 - acc: 0.977 - ETA: 0s - loss: 0.0412 - acc: 0.978 - ETA: 0s - loss: 0.0401 - acc: 0.979 - 5s 166ms/step - loss: 0.0388 - acc: 0.9800 - val_loss: 0.8220 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83333\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0098 - acc: 1.000 - ETA: 4s - loss: 0.0057 - acc: 1.000 - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 4s - loss: 0.0105 - acc: 1.000 - ETA: 3s - loss: 0.0092 - acc: 1.000 - ETA: 3s - loss: 0.0123 - acc: 1.000 - ETA: 3s - loss: 0.0111 - acc: 1.000 - ETA: 3s - loss: 0.0111 - acc: 1.000 - ETA: 3s - loss: 0.0101 - acc: 1.000 - ETA: 3s - loss: 0.0095 - acc: 1.000 - ETA: 3s - loss: 0.0088 - acc: 1.000 - ETA: 2s - loss: 0.0081 - acc: 1.000 - ETA: 2s - loss: 0.0112 - acc: 1.000 - ETA: 2s - loss: 0.0107 - acc: 1.000 - ETA: 2s - loss: 0.0100 - acc: 1.000 - ETA: 2s - loss: 0.0094 - acc: 1.000 - ETA: 2s - loss: 0.0096 - acc: 1.000 - ETA: 1s - loss: 0.0092 - acc: 1.000 - ETA: 1s - loss: 0.0087 - acc: 1.000 - ETA: 1s - loss: 0.0084 - acc: 1.000 - ETA: 1s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0077 - acc: 1.000 - ETA: 1s - loss: 0.0074 - acc: 1.000 - ETA: 0s - loss: 0.0072 - acc: 1.000 - ETA: 0s - loss: 0.0069 - acc: 1.000 - ETA: 0s - loss: 0.0225 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0286 - acc: 0.993 - 5s 165ms/step - loss: 0.0463 - acc: 0.9867 - val_loss: 0.9960 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83333\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0311 - acc: 1.000 - ETA: 4s - loss: 0.0160 - acc: 1.000 - ETA: 4s - loss: 0.0122 - acc: 1.000 - ETA: 4s - loss: 0.0131 - acc: 1.000 - ETA: 3s - loss: 0.0290 - acc: 1.000 - ETA: 3s - loss: 0.0298 - acc: 1.000 - ETA: 3s - loss: 0.0315 - acc: 1.000 - ETA: 3s - loss: 0.0450 - acc: 0.975 - ETA: 3s - loss: 0.0416 - acc: 0.977 - ETA: 3s - loss: 0.0379 - acc: 0.980 - ETA: 3s - loss: 0.0389 - acc: 0.981 - ETA: 2s - loss: 0.0370 - acc: 0.983 - ETA: 2s - loss: 0.0582 - acc: 0.969 - ETA: 2s - loss: 0.0582 - acc: 0.971 - ETA: 2s - loss: 0.0569 - acc: 0.973 - ETA: 2s - loss: 0.0540 - acc: 0.975 - ETA: 2s - loss: 0.0508 - acc: 0.976 - ETA: 1s - loss: 0.0488 - acc: 0.977 - ETA: 1s - loss: 0.0464 - acc: 0.978 - ETA: 1s - loss: 0.0444 - acc: 0.980 - ETA: 1s - loss: 0.0430 - acc: 0.981 - ETA: 1s - loss: 0.0411 - acc: 0.981 - ETA: 1s - loss: 0.0397 - acc: 0.982 - ETA: 0s - loss: 0.0381 - acc: 0.983 - ETA: 0s - loss: 0.0374 - acc: 0.984 - ETA: 0s - loss: 0.0502 - acc: 0.976 - ETA: 0s - loss: 0.0558 - acc: 0.970 - ETA: 0s - loss: 0.0539 - acc: 0.971 - ETA: 0s - loss: 0.0521 - acc: 0.972 - 5s 165ms/step - loss: 0.0504 - acc: 0.9733 - val_loss: 2.4736 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83333\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0198 - acc: 1.000 - ETA: 4s - loss: 0.3034 - acc: 0.900 - ETA: 4s - loss: 0.2038 - acc: 0.933 - ETA: 4s - loss: 0.1639 - acc: 0.950 - ETA: 3s - loss: 0.1326 - acc: 0.960 - ETA: 3s - loss: 0.1107 - acc: 0.966 - ETA: 3s - loss: 0.2679 - acc: 0.942 - ETA: 3s - loss: 0.2358 - acc: 0.950 - ETA: 3s - loss: 0.2813 - acc: 0.911 - ETA: 3s - loss: 0.2547 - acc: 0.920 - ETA: 2s - loss: 0.2495 - acc: 0.927 - ETA: 2s - loss: 0.2302 - acc: 0.933 - ETA: 2s - loss: 0.2133 - acc: 0.938 - ETA: 2s - loss: 0.2010 - acc: 0.942 - ETA: 2s - loss: 0.1880 - acc: 0.946 - ETA: 2s - loss: 0.1775 - acc: 0.950 - ETA: 2s - loss: 0.2268 - acc: 0.917 - ETA: 1s - loss: 0.2142 - acc: 0.922 - ETA: 1s - loss: 0.2030 - acc: 0.926 - ETA: 1s - loss: 0.1929 - acc: 0.930 - ETA: 1s - loss: 0.1917 - acc: 0.923 - ETA: 1s - loss: 0.1852 - acc: 0.927 - ETA: 1s - loss: 0.1980 - acc: 0.921 - ETA: 0s - loss: 0.1902 - acc: 0.925 - ETA: 0s - loss: 0.1838 - acc: 0.928 - ETA: 0s - loss: 0.1769 - acc: 0.930 - ETA: 0s - loss: 0.1712 - acc: 0.933 - ETA: 0s - loss: 0.1693 - acc: 0.935 - ETA: 0s - loss: 0.1924 - acc: 0.924 - 5s 166ms/step - loss: 0.1865 - acc: 0.9267 - val_loss: 1.4601 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83333\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0412 - acc: 1.000 - ETA: 4s - loss: 0.0213 - acc: 1.000 - ETA: 4s - loss: 0.0166 - acc: 1.000 - ETA: 4s - loss: 0.0126 - acc: 1.000 - ETA: 3s - loss: 0.0111 - acc: 1.000 - ETA: 3s - loss: 0.0160 - acc: 1.000 - ETA: 3s - loss: 0.0157 - acc: 1.000 - ETA: 3s - loss: 0.0176 - acc: 1.000 - ETA: 3s - loss: 0.0165 - acc: 1.000 - ETA: 3s - loss: 0.0485 - acc: 0.980 - ETA: 3s - loss: 0.0514 - acc: 0.981 - ETA: 2s - loss: 0.0517 - acc: 0.983 - ETA: 2s - loss: 0.0478 - acc: 0.984 - ETA: 2s - loss: 0.0781 - acc: 0.957 - ETA: 2s - loss: 0.0731 - acc: 0.960 - ETA: 2s - loss: 0.1324 - acc: 0.937 - ETA: 2s - loss: 0.1344 - acc: 0.929 - ETA: 1s - loss: 0.1294 - acc: 0.933 - ETA: 1s - loss: 0.1235 - acc: 0.936 - ETA: 1s - loss: 0.1509 - acc: 0.930 - ETA: 1s - loss: 0.1988 - acc: 0.914 - ETA: 1s - loss: 0.1904 - acc: 0.918 - ETA: 1s - loss: 0.1828 - acc: 0.921 - ETA: 0s - loss: 0.1884 - acc: 0.916 - ETA: 0s - loss: 0.1892 - acc: 0.912 - ETA: 0s - loss: 0.1823 - acc: 0.915 - ETA: 0s - loss: 0.1801 - acc: 0.918 - ETA: 0s - loss: 0.1744 - acc: 0.921 - ETA: 0s - loss: 0.1996 - acc: 0.917 - 5s 166ms/step - loss: 0.2311 - acc: 0.9133 - val_loss: 1.5279 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83333\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1097 - acc: 1.000 - ETA: 4s - loss: 0.0664 - acc: 1.000 - ETA: 4s - loss: 0.0509 - acc: 1.000 - ETA: 4s - loss: 0.0439 - acc: 1.000 - ETA: 3s - loss: 0.0418 - acc: 1.000 - ETA: 3s - loss: 0.0742 - acc: 1.000 - ETA: 3s - loss: 0.0664 - acc: 1.000 - ETA: 3s - loss: 0.0769 - acc: 0.975 - ETA: 3s - loss: 0.1240 - acc: 0.933 - ETA: 3s - loss: 0.1118 - acc: 0.940 - ETA: 2s - loss: 0.1092 - acc: 0.945 - ETA: 2s - loss: 0.1232 - acc: 0.933 - ETA: 2s - loss: 0.1143 - acc: 0.938 - ETA: 2s - loss: 0.1067 - acc: 0.942 - ETA: 2s - loss: 0.1054 - acc: 0.946 - ETA: 2s - loss: 0.1253 - acc: 0.937 - ETA: 2s - loss: 0.1182 - acc: 0.941 - ETA: 1s - loss: 0.1165 - acc: 0.944 - ETA: 1s - loss: 0.1106 - acc: 0.947 - ETA: 1s - loss: 0.1452 - acc: 0.930 - ETA: 1s - loss: 0.1383 - acc: 0.933 - ETA: 1s - loss: 0.1325 - acc: 0.936 - ETA: 1s - loss: 0.1270 - acc: 0.939 - ETA: 0s - loss: 0.1220 - acc: 0.941 - ETA: 0s - loss: 0.1245 - acc: 0.936 - ETA: 0s - loss: 0.1204 - acc: 0.938 - ETA: 0s - loss: 0.1186 - acc: 0.940 - ETA: 0s - loss: 0.1169 - acc: 0.942 - ETA: 0s - loss: 0.1131 - acc: 0.944 - 5s 166ms/step - loss: 0.1127 - acc: 0.9467 - val_loss: 0.8277 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83333\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0070 - acc: 1.000 - ETA: 4s - loss: 0.1472 - acc: 0.900 - ETA: 4s - loss: 0.1222 - acc: 0.933 - ETA: 4s - loss: 0.1202 - acc: 0.950 - ETA: 3s - loss: 0.1940 - acc: 0.880 - ETA: 3s - loss: 0.1939 - acc: 0.900 - ETA: 3s - loss: 0.1666 - acc: 0.914 - ETA: 3s - loss: 0.1458 - acc: 0.925 - ETA: 3s - loss: 0.1298 - acc: 0.933 - ETA: 3s - loss: 0.1169 - acc: 0.940 - ETA: 3s - loss: 0.1064 - acc: 0.945 - ETA: 2s - loss: 0.1727 - acc: 0.916 - ETA: 2s - loss: 0.1597 - acc: 0.923 - ETA: 2s - loss: 0.1530 - acc: 0.928 - ETA: 2s - loss: 0.1490 - acc: 0.933 - ETA: 2s - loss: 0.1398 - acc: 0.937 - ETA: 2s - loss: 0.1443 - acc: 0.929 - ETA: 1s - loss: 0.1807 - acc: 0.922 - ETA: 1s - loss: 0.1718 - acc: 0.926 - ETA: 1s - loss: 0.1692 - acc: 0.930 - ETA: 1s - loss: 0.1629 - acc: 0.933 - ETA: 1s - loss: 0.1576 - acc: 0.936 - ETA: 1s - loss: 0.1513 - acc: 0.939 - ETA: 0s - loss: 0.1462 - acc: 0.941 - ETA: 0s - loss: 0.1417 - acc: 0.944 - ETA: 0s - loss: 0.1368 - acc: 0.946 - ETA: 0s - loss: 0.1324 - acc: 0.948 - ETA: 0s - loss: 0.1278 - acc: 0.950 - ETA: 0s - loss: 0.1255 - acc: 0.951 - 5s 166ms/step - loss: 0.1244 - acc: 0.9533 - val_loss: 1.8764 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83333\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 4s - loss: 0.0125 - acc: 1.000 - ETA: 4s - loss: 0.1184 - acc: 0.933 - ETA: 4s - loss: 0.1915 - acc: 0.900 - ETA: 3s - loss: 0.1540 - acc: 0.920 - ETA: 3s - loss: 0.1292 - acc: 0.933 - ETA: 3s - loss: 0.1171 - acc: 0.942 - ETA: 3s - loss: 0.1026 - acc: 0.950 - ETA: 3s - loss: 0.1574 - acc: 0.933 - ETA: 3s - loss: 0.1512 - acc: 0.940 - ETA: 3s - loss: 0.1434 - acc: 0.945 - ETA: 2s - loss: 0.1315 - acc: 0.950 - ETA: 2s - loss: 0.1236 - acc: 0.953 - ETA: 2s - loss: 0.1150 - acc: 0.957 - ETA: 2s - loss: 0.1100 - acc: 0.960 - ETA: 2s - loss: 0.1041 - acc: 0.962 - ETA: 2s - loss: 0.0984 - acc: 0.964 - ETA: 1s - loss: 0.0945 - acc: 0.966 - ETA: 1s - loss: 0.0896 - acc: 0.968 - ETA: 1s - loss: 0.0853 - acc: 0.970 - ETA: 1s - loss: 0.0824 - acc: 0.971 - ETA: 1s - loss: 0.0856 - acc: 0.972 - ETA: 1s - loss: 0.0819 - acc: 0.973 - ETA: 0s - loss: 0.0787 - acc: 0.975 - ETA: 0s - loss: 0.0757 - acc: 0.976 - ETA: 0s - loss: 0.0983 - acc: 0.969 - ETA: 0s - loss: 0.0958 - acc: 0.970 - ETA: 0s - loss: 0.0924 - acc: 0.971 - ETA: 0s - loss: 0.0905 - acc: 0.972 - 5s 166ms/step - loss: 0.0893 - acc: 0.9733 - val_loss: 0.7341 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83333\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 4s - loss: 0.0026 - acc: 1.000 - ETA: 4s - loss: 0.0052 - acc: 1.000 - ETA: 4s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0060 - acc: 1.000 - ETA: 3s - loss: 0.0139 - acc: 1.000 - ETA: 3s - loss: 0.2063 - acc: 0.942 - ETA: 3s - loss: 0.1807 - acc: 0.950 - ETA: 3s - loss: 0.1610 - acc: 0.955 - ETA: 3s - loss: 0.1450 - acc: 0.960 - ETA: 2s - loss: 0.1323 - acc: 0.963 - ETA: 2s - loss: 0.1214 - acc: 0.966 - ETA: 2s - loss: 0.1126 - acc: 0.969 - ETA: 2s - loss: 0.1071 - acc: 0.971 - ETA: 2s - loss: 0.1259 - acc: 0.960 - ETA: 2s - loss: 0.1182 - acc: 0.962 - ETA: 2s - loss: 0.1114 - acc: 0.964 - ETA: 1s - loss: 0.1060 - acc: 0.966 - ETA: 1s - loss: 0.1005 - acc: 0.968 - ETA: 1s - loss: 0.0954 - acc: 0.970 - ETA: 1s - loss: 0.0911 - acc: 0.971 - ETA: 1s - loss: 0.0898 - acc: 0.972 - ETA: 1s - loss: 0.0881 - acc: 0.973 - ETA: 0s - loss: 0.0845 - acc: 0.975 - ETA: 0s - loss: 0.0825 - acc: 0.976 - ETA: 0s - loss: 0.1048 - acc: 0.969 - ETA: 0s - loss: 0.1089 - acc: 0.963 - ETA: 0s - loss: 0.1051 - acc: 0.964 - ETA: 0s - loss: 0.1070 - acc: 0.958 - 5s 165ms/step - loss: 0.1035 - acc: 0.9600 - val_loss: 1.9009 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.83333\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2848 - acc: 0.800 - ETA: 4s - loss: 0.2664 - acc: 0.800 - ETA: 4s - loss: 0.2954 - acc: 0.800 - ETA: 4s - loss: 0.2222 - acc: 0.850 - ETA: 3s - loss: 0.1784 - acc: 0.880 - ETA: 3s - loss: 0.1490 - acc: 0.900 - ETA: 3s - loss: 0.1550 - acc: 0.914 - ETA: 3s - loss: 0.1358 - acc: 0.925 - ETA: 3s - loss: 0.2440 - acc: 0.888 - ETA: 3s - loss: 0.2202 - acc: 0.900 - ETA: 2s - loss: 0.2003 - acc: 0.909 - ETA: 2s - loss: 0.1852 - acc: 0.916 - ETA: 2s - loss: 0.2200 - acc: 0.892 - ETA: 2s - loss: 0.2043 - acc: 0.900 - ETA: 2s - loss: 0.2061 - acc: 0.893 - ETA: 2s - loss: 0.1944 - acc: 0.900 - ETA: 2s - loss: 0.1863 - acc: 0.905 - ETA: 1s - loss: 0.2051 - acc: 0.900 - ETA: 1s - loss: 0.1947 - acc: 0.905 - ETA: 1s - loss: 0.1922 - acc: 0.910 - ETA: 1s - loss: 0.1899 - acc: 0.904 - ETA: 1s - loss: 0.1830 - acc: 0.909 - ETA: 1s - loss: 0.1784 - acc: 0.913 - ETA: 0s - loss: 0.1765 - acc: 0.916 - ETA: 0s - loss: 0.1777 - acc: 0.912 - ETA: 0s - loss: 0.1709 - acc: 0.915 - ETA: 0s - loss: 0.1675 - acc: 0.918 - ETA: 0s - loss: 0.1630 - acc: 0.921 - ETA: 0s - loss: 0.1788 - acc: 0.910 - 5s 165ms/step - loss: 0.1807 - acc: 0.9067 - val_loss: 1.4311 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83333\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0031 - acc: 1.000 - ETA: 4s - loss: 0.1908 - acc: 0.900 - ETA: 4s - loss: 0.1439 - acc: 0.933 - ETA: 4s - loss: 0.1269 - acc: 0.950 - ETA: 3s - loss: 0.1184 - acc: 0.960 - ETA: 3s - loss: 0.1906 - acc: 0.933 - ETA: 3s - loss: 0.2053 - acc: 0.914 - ETA: 3s - loss: 0.1811 - acc: 0.925 - ETA: 3s - loss: 0.1667 - acc: 0.933 - ETA: 3s - loss: 0.1506 - acc: 0.940 - ETA: 3s - loss: 0.1379 - acc: 0.945 - ETA: 2s - loss: 0.1276 - acc: 0.950 - ETA: 2s - loss: 0.1183 - acc: 0.953 - ETA: 2s - loss: 0.1113 - acc: 0.957 - ETA: 2s - loss: 0.1041 - acc: 0.960 - ETA: 2s - loss: 0.1122 - acc: 0.950 - ETA: 2s - loss: 0.1060 - acc: 0.952 - ETA: 1s - loss: 0.1041 - acc: 0.955 - ETA: 1s - loss: 0.1723 - acc: 0.936 - ETA: 1s - loss: 0.1637 - acc: 0.940 - ETA: 1s - loss: 0.1561 - acc: 0.942 - ETA: 1s - loss: 0.1498 - acc: 0.945 - ETA: 1s - loss: 0.1535 - acc: 0.939 - ETA: 0s - loss: 0.1471 - acc: 0.941 - ETA: 0s - loss: 0.1462 - acc: 0.944 - ETA: 0s - loss: 0.1410 - acc: 0.946 - ETA: 0s - loss: 0.1361 - acc: 0.948 - ETA: 0s - loss: 0.1319 - acc: 0.950 - ETA: 0s - loss: 0.1276 - acc: 0.951 - 5s 166ms/step - loss: 0.1233 - acc: 0.9533 - val_loss: 1.7402 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83333\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.4529 - acc: 0.800 - ETA: 4s - loss: 0.2296 - acc: 0.900 - ETA: 4s - loss: 0.1584 - acc: 0.933 - ETA: 4s - loss: 0.1316 - acc: 0.950 - ETA: 3s - loss: 0.1093 - acc: 0.960 - ETA: 3s - loss: 0.0913 - acc: 0.966 - ETA: 3s - loss: 0.0798 - acc: 0.971 - ETA: 3s - loss: 0.0842 - acc: 0.975 - ETA: 3s - loss: 0.0749 - acc: 0.977 - ETA: 3s - loss: 0.1601 - acc: 0.960 - ETA: 2s - loss: 0.1465 - acc: 0.963 - ETA: 2s - loss: 0.1346 - acc: 0.966 - ETA: 2s - loss: 0.1243 - acc: 0.969 - ETA: 2s - loss: 0.1154 - acc: 0.971 - ETA: 2s - loss: 0.1090 - acc: 0.973 - ETA: 2s - loss: 0.1022 - acc: 0.975 - ETA: 2s - loss: 0.0963 - acc: 0.976 - ETA: 1s - loss: 0.0917 - acc: 0.977 - ETA: 1s - loss: 0.0869 - acc: 0.978 - ETA: 1s - loss: 0.0828 - acc: 0.980 - ETA: 1s - loss: 0.0793 - acc: 0.981 - ETA: 1s - loss: 0.0987 - acc: 0.963 - ETA: 1s - loss: 0.0955 - acc: 0.965 - ETA: 0s - loss: 0.0918 - acc: 0.966 - ETA: 0s - loss: 0.0881 - acc: 0.968 - ETA: 0s - loss: 0.0851 - acc: 0.969 - ETA: 0s - loss: 0.0821 - acc: 0.970 - ETA: 0s - loss: 0.1121 - acc: 0.957 - ETA: 0s - loss: 0.1431 - acc: 0.951 - 5s 166ms/step - loss: 0.1403 - acc: 0.9533 - val_loss: 1.5046 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83333\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0320 - acc: 1.000 - ETA: 4s - loss: 0.0202 - acc: 1.000 - ETA: 4s - loss: 0.0163 - acc: 1.000 - ETA: 4s - loss: 0.1125 - acc: 0.950 - ETA: 3s - loss: 0.0918 - acc: 0.960 - ETA: 3s - loss: 0.0784 - acc: 0.966 - ETA: 3s - loss: 0.0734 - acc: 0.971 - ETA: 3s - loss: 0.0646 - acc: 0.975 - ETA: 3s - loss: 0.0581 - acc: 0.977 - ETA: 3s - loss: 0.0537 - acc: 0.980 - ETA: 3s - loss: 0.0505 - acc: 0.981 - ETA: 2s - loss: 0.0475 - acc: 0.983 - ETA: 2s - loss: 0.0440 - acc: 0.984 - ETA: 2s - loss: 0.0421 - acc: 0.985 - ETA: 2s - loss: 0.0424 - acc: 0.986 - ETA: 2s - loss: 0.0482 - acc: 0.987 - ETA: 2s - loss: 0.0469 - acc: 0.988 - ETA: 1s - loss: 0.0447 - acc: 0.988 - ETA: 1s - loss: 0.0501 - acc: 0.989 - ETA: 1s - loss: 0.0479 - acc: 0.990 - ETA: 1s - loss: 0.0458 - acc: 0.990 - ETA: 1s - loss: 0.0470 - acc: 0.990 - ETA: 1s - loss: 0.0587 - acc: 0.982 - ETA: 0s - loss: 0.0582 - acc: 0.983 - ETA: 0s - loss: 0.0603 - acc: 0.984 - ETA: 0s - loss: 0.0580 - acc: 0.984 - ETA: 0s - loss: 0.0559 - acc: 0.985 - ETA: 0s - loss: 0.0540 - acc: 0.985 - ETA: 0s - loss: 0.0521 - acc: 0.986 - 5s 166ms/step - loss: 0.0506 - acc: 0.9867 - val_loss: 0.9499 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83333\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.4185e-04 - acc: 1.000 - ETA: 4s - loss: 0.0057 - acc: 1.0000    - ETA: 4s - loss: 0.0040 - acc: 1.000 - ETA: 4s - loss: 0.0071 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0072 - acc: 1.000 - ETA: 3s - loss: 0.0092 - acc: 1.000 - ETA: 3s - loss: 0.0103 - acc: 1.000 - ETA: 3s - loss: 0.0103 - acc: 1.000 - ETA: 3s - loss: 0.0093 - acc: 1.000 - ETA: 3s - loss: 0.0086 - acc: 1.000 - ETA: 2s - loss: 0.0209 - acc: 1.000 - ETA: 2s - loss: 0.0193 - acc: 1.000 - ETA: 2s - loss: 0.0179 - acc: 1.000 - ETA: 2s - loss: 0.0316 - acc: 0.986 - ETA: 2s - loss: 0.0344 - acc: 0.987 - ETA: 2s - loss: 0.0325 - acc: 0.988 - ETA: 1s - loss: 0.0308 - acc: 0.988 - ETA: 1s - loss: 0.0293 - acc: 0.989 - ETA: 1s - loss: 0.0282 - acc: 0.990 - ETA: 1s - loss: 0.0272 - acc: 0.990 - ETA: 1s - loss: 0.0261 - acc: 0.990 - ETA: 1s - loss: 0.0249 - acc: 0.991 - ETA: 0s - loss: 0.0247 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0395 - acc: 0.986 - 5s 166ms/step - loss: 0.0382 - acc: 0.9867 - val_loss: 0.7591 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83333\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.0206e-04 - acc: 1.000 - ETA: 4s - loss: 5.4664e-04 - acc: 1.000 - ETA: 4s - loss: 3.8671e-04 - acc: 1.000 - ETA: 4s - loss: 4.3013e-04 - acc: 1.000 - ETA: 4s - loss: 3.5080e-04 - acc: 1.000 - ETA: 3s - loss: 4.5743e-04 - acc: 1.000 - ETA: 3s - loss: 8.2688e-04 - acc: 1.000 - ETA: 3s - loss: 8.4383e-04 - acc: 1.000 - ETA: 3s - loss: 8.2301e-04 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.0000    - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0304 - acc: 0.987 - ETA: 2s - loss: 0.0691 - acc: 0.964 - ETA: 1s - loss: 0.0660 - acc: 0.966 - ETA: 1s - loss: 0.0639 - acc: 0.968 - ETA: 1s - loss: 0.0607 - acc: 0.970 - ETA: 1s - loss: 0.0668 - acc: 0.961 - ETA: 1s - loss: 0.0638 - acc: 0.963 - ETA: 1s - loss: 0.0617 - acc: 0.965 - ETA: 0s - loss: 0.0747 - acc: 0.958 - ETA: 0s - loss: 0.0717 - acc: 0.960 - ETA: 0s - loss: 0.0693 - acc: 0.961 - ETA: 0s - loss: 0.0668 - acc: 0.963 - ETA: 0s - loss: 0.0647 - acc: 0.964 - ETA: 0s - loss: 0.0626 - acc: 0.965 - 5s 166ms/step - loss: 0.0615 - acc: 0.9667 - val_loss: 2.9049 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.83333\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0060 - acc: 1.000 - ETA: 4s - loss: 0.0678 - acc: 1.000 - ETA: 4s - loss: 0.0460 - acc: 1.000 - ETA: 4s - loss: 0.0369 - acc: 1.000 - ETA: 4s - loss: 0.0300 - acc: 1.000 - ETA: 3s - loss: 0.0257 - acc: 1.000 - ETA: 3s - loss: 0.0363 - acc: 1.000 - ETA: 3s - loss: 0.0325 - acc: 1.000 - ETA: 3s - loss: 0.0290 - acc: 1.000 - ETA: 3s - loss: 0.0262 - acc: 1.000 - ETA: 3s - loss: 0.0247 - acc: 1.000 - ETA: 2s - loss: 0.0227 - acc: 1.000 - ETA: 2s - loss: 0.0210 - acc: 1.000 - ETA: 2s - loss: 0.0204 - acc: 1.000 - ETA: 2s - loss: 0.0204 - acc: 1.000 - ETA: 2s - loss: 0.0247 - acc: 1.000 - ETA: 2s - loss: 0.0244 - acc: 1.000 - ETA: 1s - loss: 0.0334 - acc: 0.988 - ETA: 1s - loss: 0.0514 - acc: 0.978 - ETA: 1s - loss: 0.0529 - acc: 0.980 - ETA: 1s - loss: 0.0504 - acc: 0.981 - ETA: 1s - loss: 0.0485 - acc: 0.981 - ETA: 1s - loss: 0.0465 - acc: 0.982 - ETA: 0s - loss: 0.0468 - acc: 0.983 - ETA: 0s - loss: 0.0452 - acc: 0.984 - ETA: 0s - loss: 0.0435 - acc: 0.984 - ETA: 0s - loss: 0.0441 - acc: 0.985 - ETA: 0s - loss: 0.0428 - acc: 0.985 - ETA: 0s - loss: 0.0417 - acc: 0.986 - 5s 166ms/step - loss: 0.0519 - acc: 0.9800 - val_loss: 1.5948 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.83333\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0196 - acc: 1.000 - ETA: 4s - loss: 0.0113 - acc: 1.000 - ETA: 4s - loss: 0.0080 - acc: 1.000 - ETA: 4s - loss: 0.0080 - acc: 1.000 - ETA: 3s - loss: 0.0148 - acc: 1.000 - ETA: 3s - loss: 0.0133 - acc: 1.000 - ETA: 3s - loss: 0.0114 - acc: 1.000 - ETA: 3s - loss: 0.0110 - acc: 1.000 - ETA: 3s - loss: 0.0101 - acc: 1.000 - ETA: 3s - loss: 0.0107 - acc: 1.000 - ETA: 3s - loss: 0.0103 - acc: 1.000 - ETA: 2s - loss: 0.0222 - acc: 1.000 - ETA: 2s - loss: 0.0207 - acc: 1.000 - ETA: 2s - loss: 0.0194 - acc: 1.000 - ETA: 2s - loss: 0.0240 - acc: 1.000 - ETA: 2s - loss: 0.0226 - acc: 1.000 - ETA: 2s - loss: 0.0226 - acc: 1.000 - ETA: 1s - loss: 0.0215 - acc: 1.000 - ETA: 1s - loss: 0.0204 - acc: 1.000 - ETA: 1s - loss: 0.0194 - acc: 1.000 - ETA: 1s - loss: 0.0198 - acc: 1.000 - ETA: 1s - loss: 0.0190 - acc: 1.000 - ETA: 1s - loss: 0.0196 - acc: 1.000 - ETA: 0s - loss: 0.0189 - acc: 1.000 - ETA: 0s - loss: 0.0182 - acc: 1.000 - ETA: 0s - loss: 0.0192 - acc: 1.000 - ETA: 0s - loss: 0.0185 - acc: 1.000 - ETA: 0s - loss: 0.0237 - acc: 1.000 - ETA: 0s - loss: 0.0246 - acc: 1.000 - 5s 165ms/step - loss: 0.0238 - acc: 1.0000 - val_loss: 1.1608 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.83333\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0027 - acc: 1.000 - ETA: 4s - loss: 0.0016 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 0.0063 - acc: 1.000 - ETA: 3s - loss: 0.0055 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0055 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 2s - loss: 0.0041 - acc: 1.000 - ETA: 2s - loss: 0.0038 - acc: 1.000 - ETA: 2s - loss: 0.0288 - acc: 0.984 - ETA: 2s - loss: 0.0267 - acc: 0.985 - ETA: 2s - loss: 0.0257 - acc: 0.986 - ETA: 2s - loss: 0.0255 - acc: 0.987 - ETA: 2s - loss: 0.0240 - acc: 0.988 - ETA: 1s - loss: 0.0228 - acc: 0.988 - ETA: 1s - loss: 0.0217 - acc: 0.989 - ETA: 1s - loss: 0.0209 - acc: 0.990 - ETA: 1s - loss: 0.0212 - acc: 0.990 - ETA: 1s - loss: 0.0239 - acc: 0.990 - ETA: 1s - loss: 0.0229 - acc: 0.991 - ETA: 0s - loss: 0.0239 - acc: 0.991 - ETA: 0s - loss: 0.0231 - acc: 0.992 - ETA: 0s - loss: 0.0228 - acc: 0.992 - ETA: 0s - loss: 0.0220 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0248 - acc: 0.993 - 5s 166ms/step - loss: 0.0240 - acc: 0.9933 - val_loss: 0.8139 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.83333\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0040 - acc: 1.000 - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 4s - loss: 0.0019 - acc: 1.000 - ETA: 4s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 3s - loss: 0.0024 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0079 - acc: 1.000 - ETA: 3s - loss: 0.0071 - acc: 1.000 - ETA: 2s - loss: 0.0066 - acc: 1.000 - ETA: 2s - loss: 0.0061 - acc: 1.000 - ETA: 2s - loss: 0.0057 - acc: 1.000 - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 2s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0072 - acc: 1.000 - ETA: 2s - loss: 0.0069 - acc: 1.000 - ETA: 1s - loss: 0.0065 - acc: 1.000 - ETA: 1s - loss: 0.0061 - acc: 1.000 - ETA: 1s - loss: 0.0059 - acc: 1.000 - ETA: 1s - loss: 0.0057 - acc: 1.000 - ETA: 1s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0051 - acc: 1.000 - ETA: 0s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 0s - loss: 0.0054 - acc: 1.000 - 5s 166ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.8509 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.83333 to 0.86667, saving model to C:\\Users\\lizzi\\OneDrive\\UNI\\2019\\thesis\\Machine Learning Course\\YWLAI\\rex\\rex_models\\rex_weight_model.063-0.8666666746139526.h5\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.0884e-05 - acc: 1.000 - ETA: 4s - loss: 4.1482e-05 - acc: 1.000 - ETA: 4s - loss: 7.0408e-05 - acc: 1.000 - ETA: 4s - loss: 1.9270e-04 - acc: 1.000 - ETA: 3s - loss: 1.7359e-04 - acc: 1.000 - ETA: 3s - loss: 7.2411e-04 - acc: 1.000 - ETA: 3s - loss: 6.5120e-04 - acc: 1.000 - ETA: 3s - loss: 5.7606e-04 - acc: 1.000 - ETA: 3s - loss: 5.8750e-04 - acc: 1.000 - ETA: 3s - loss: 5.3826e-04 - acc: 1.000 - ETA: 2s - loss: 5.0222e-04 - acc: 1.000 - ETA: 2s - loss: 4.8855e-04 - acc: 1.000 - ETA: 2s - loss: 5.1990e-04 - acc: 1.000 - ETA: 2s - loss: 5.8907e-04 - acc: 1.000 - ETA: 2s - loss: 6.3199e-04 - acc: 1.000 - ETA: 2s - loss: 6.4936e-04 - acc: 1.000 - ETA: 2s - loss: 6.3710e-04 - acc: 1.000 - ETA: 1s - loss: 0.0083 - acc: 1.0000    - ETA: 1s - loss: 0.0079 - acc: 1.000 - ETA: 1s - loss: 0.0091 - acc: 1.000 - ETA: 1s - loss: 0.0087 - acc: 1.000 - ETA: 1s - loss: 0.0083 - acc: 1.000 - ETA: 1s - loss: 0.0079 - acc: 1.000 - ETA: 0s - loss: 0.0076 - acc: 1.000 - ETA: 0s - loss: 0.0073 - acc: 1.000 - ETA: 0s - loss: 0.0070 - acc: 1.000 - ETA: 0s - loss: 0.0084 - acc: 1.000 - ETA: 0s - loss: 0.0083 - acc: 1.000 - ETA: 0s - loss: 0.0082 - acc: 1.000 - 5s 166ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 1.1855 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86667\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 5.8059e-05 - acc: 1.000 - ETA: 4s - loss: 0.0017 - acc: 1.0000    - ETA: 4s - loss: 0.0023 - acc: 1.000 - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0086 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0070 - acc: 1.000 - ETA: 3s - loss: 0.0085 - acc: 1.000 - ETA: 3s - loss: 0.0077 - acc: 1.000 - ETA: 3s - loss: 0.0070 - acc: 1.000 - ETA: 2s - loss: 0.0064 - acc: 1.000 - ETA: 2s - loss: 0.0062 - acc: 1.000 - ETA: 2s - loss: 0.0059 - acc: 1.000 - ETA: 2s - loss: 0.0055 - acc: 1.000 - ETA: 2s - loss: 0.0052 - acc: 1.000 - ETA: 2s - loss: 0.0050 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 1.000 - ETA: 1s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0058 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 1.000 - ETA: 0s - loss: 0.0054 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 0s - loss: 0.0050 - acc: 1.000 - ETA: 0s - loss: 0.0048 - acc: 1.000 - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 0s - loss: 0.0045 - acc: 1.000 - 5s 166ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9441 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.86667\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.4513e-04 - acc: 1.000 - ETA: 4s - loss: 3.0758e-04 - acc: 1.000 - ETA: 4s - loss: 0.0035 - acc: 1.0000    - ETA: 4s - loss: 0.0027 - acc: 1.000 - ETA: 3s - loss: 0.0051 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0078 - acc: 1.000 - ETA: 3s - loss: 0.0075 - acc: 1.000 - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 3s - loss: 0.0068 - acc: 1.000 - ETA: 3s - loss: 0.0062 - acc: 1.000 - ETA: 2s - loss: 0.0058 - acc: 1.000 - ETA: 2s - loss: 0.0053 - acc: 1.000 - ETA: 2s - loss: 0.0054 - acc: 1.000 - ETA: 2s - loss: 0.0050 - acc: 1.000 - ETA: 2s - loss: 0.0047 - acc: 1.000 - ETA: 2s - loss: 0.0045 - acc: 1.000 - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0042 - acc: 1.000 - ETA: 1s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0033 - acc: 1.000 - ETA: 0s - loss: 0.0032 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0030 - acc: 1.000 - ETA: 0s - loss: 0.0029 - acc: 1.000 - ETA: 0s - loss: 0.0032 - acc: 1.000 - 5s 165ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.9796 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.86667\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 3.1466e-04 - acc: 1.000 - ETA: 4s - loss: 2.2248e-04 - acc: 1.000 - ETA: 4s - loss: 4.8031e-04 - acc: 1.000 - ETA: 4s - loss: 0.0059 - acc: 1.0000    - ETA: 3s - loss: 0.0060 - acc: 1.000 - ETA: 3s - loss: 0.0051 - acc: 1.000 - ETA: 3s - loss: 0.0048 - acc: 1.000 - ETA: 3s - loss: 0.0058 - acc: 1.000 - ETA: 3s - loss: 0.0052 - acc: 1.000 - ETA: 3s - loss: 0.0047 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0039 - acc: 1.000 - ETA: 2s - loss: 0.0043 - acc: 1.000 - ETA: 2s - loss: 0.0041 - acc: 1.000 - ETA: 2s - loss: 0.0358 - acc: 0.987 - ETA: 2s - loss: 0.0337 - acc: 0.988 - ETA: 1s - loss: 0.0318 - acc: 0.988 - ETA: 1s - loss: 0.0302 - acc: 0.989 - ETA: 1s - loss: 0.0413 - acc: 0.980 - ETA: 1s - loss: 0.0393 - acc: 0.981 - ETA: 1s - loss: 0.0415 - acc: 0.981 - ETA: 1s - loss: 0.0397 - acc: 0.982 - ETA: 0s - loss: 0.0381 - acc: 0.983 - ETA: 0s - loss: 0.0366 - acc: 0.984 - ETA: 0s - loss: 0.0359 - acc: 0.984 - ETA: 0s - loss: 0.0348 - acc: 0.985 - ETA: 0s - loss: 0.0379 - acc: 0.985 - ETA: 0s - loss: 0.0367 - acc: 0.986 - 5s 165ms/step - loss: 0.0358 - acc: 0.9867 - val_loss: 1.1408 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86667\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 0.1001 - acc: 0.900 - ETA: 4s - loss: 0.0682 - acc: 0.933 - ETA: 4s - loss: 0.0519 - acc: 0.950 - ETA: 3s - loss: 0.0503 - acc: 0.960 - ETA: 3s - loss: 0.0423 - acc: 0.966 - ETA: 3s - loss: 0.0405 - acc: 0.971 - ETA: 3s - loss: 0.0355 - acc: 0.975 - ETA: 3s - loss: 0.0393 - acc: 0.977 - ETA: 3s - loss: 0.1075 - acc: 0.960 - ETA: 2s - loss: 0.0978 - acc: 0.963 - ETA: 2s - loss: 0.0896 - acc: 0.966 - ETA: 2s - loss: 0.0828 - acc: 0.969 - ETA: 2s - loss: 0.0788 - acc: 0.971 - ETA: 2s - loss: 0.0788 - acc: 0.973 - ETA: 2s - loss: 0.0749 - acc: 0.975 - ETA: 2s - loss: 0.0737 - acc: 0.976 - ETA: 1s - loss: 0.0772 - acc: 0.977 - ETA: 1s - loss: 0.0732 - acc: 0.978 - ETA: 1s - loss: 0.0696 - acc: 0.980 - ETA: 1s - loss: 0.0664 - acc: 0.981 - ETA: 1s - loss: 0.0671 - acc: 0.981 - ETA: 1s - loss: 0.0643 - acc: 0.982 - ETA: 0s - loss: 0.0617 - acc: 0.983 - ETA: 0s - loss: 0.0592 - acc: 0.984 - ETA: 0s - loss: 0.0570 - acc: 0.984 - ETA: 0s - loss: 0.0549 - acc: 0.985 - ETA: 0s - loss: 0.0531 - acc: 0.985 - ETA: 0s - loss: 0.0512 - acc: 0.986 - 5s 165ms/step - loss: 0.0496 - acc: 0.9867 - val_loss: 2.7851 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86667\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 5.2689e-04 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.0000    - ETA: 4s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 0.0166 - acc: 1.000 - ETA: 3s - loss: 0.0137 - acc: 1.000 - ETA: 3s - loss: 0.0115 - acc: 1.000 - ETA: 3s - loss: 0.0334 - acc: 0.971 - ETA: 3s - loss: 0.0318 - acc: 0.975 - ETA: 3s - loss: 0.0297 - acc: 0.977 - ETA: 3s - loss: 0.0267 - acc: 0.980 - ETA: 3s - loss: 0.0814 - acc: 0.963 - ETA: 2s - loss: 0.0762 - acc: 0.966 - ETA: 2s - loss: 0.0706 - acc: 0.969 - ETA: 2s - loss: 0.0772 - acc: 0.957 - ETA: 2s - loss: 0.0722 - acc: 0.960 - ETA: 2s - loss: 0.0683 - acc: 0.962 - ETA: 2s - loss: 0.0704 - acc: 0.964 - ETA: 1s - loss: 0.0867 - acc: 0.955 - ETA: 1s - loss: 0.0827 - acc: 0.957 - ETA: 1s - loss: 0.0786 - acc: 0.960 - ETA: 1s - loss: 0.1260 - acc: 0.942 - ETA: 1s - loss: 0.1444 - acc: 0.936 - ETA: 1s - loss: 0.1382 - acc: 0.939 - ETA: 0s - loss: 0.1370 - acc: 0.941 - ETA: 0s - loss: 0.1408 - acc: 0.936 - ETA: 0s - loss: 0.1385 - acc: 0.938 - ETA: 0s - loss: 0.1333 - acc: 0.940 - ETA: 0s - loss: 0.1287 - acc: 0.942 - ETA: 0s - loss: 0.1354 - acc: 0.937 - 5s 166ms/step - loss: 0.1662 - acc: 0.9333 - val_loss: 1.9902 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.86667\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2043 - acc: 0.800 - ETA: 4s - loss: 0.1033 - acc: 0.900 - ETA: 4s - loss: 0.0952 - acc: 0.933 - ETA: 4s - loss: 0.1357 - acc: 0.900 - ETA: 3s - loss: 0.4810 - acc: 0.840 - ETA: 3s - loss: 0.4010 - acc: 0.866 - ETA: 3s - loss: 0.3450 - acc: 0.885 - ETA: 3s - loss: 0.3023 - acc: 0.900 - ETA: 3s - loss: 0.2742 - acc: 0.911 - ETA: 3s - loss: 0.2471 - acc: 0.920 - ETA: 2s - loss: 0.2247 - acc: 0.927 - ETA: 2s - loss: 0.2078 - acc: 0.933 - ETA: 2s - loss: 0.2020 - acc: 0.938 - ETA: 2s - loss: 0.1933 - acc: 0.942 - ETA: 2s - loss: 0.1808 - acc: 0.946 - ETA: 2s - loss: 0.1739 - acc: 0.950 - ETA: 2s - loss: 0.1673 - acc: 0.952 - ETA: 1s - loss: 0.1719 - acc: 0.944 - ETA: 1s - loss: 0.1635 - acc: 0.947 - ETA: 1s - loss: 0.1560 - acc: 0.950 - ETA: 1s - loss: 0.1507 - acc: 0.952 - ETA: 1s - loss: 0.1467 - acc: 0.954 - ETA: 1s - loss: 0.1414 - acc: 0.956 - ETA: 0s - loss: 0.1402 - acc: 0.958 - ETA: 0s - loss: 0.1347 - acc: 0.960 - ETA: 0s - loss: 0.1307 - acc: 0.961 - ETA: 0s - loss: 0.1265 - acc: 0.963 - ETA: 0s - loss: 0.1222 - acc: 0.964 - ETA: 0s - loss: 0.1204 - acc: 0.965 - 5s 165ms/step - loss: 0.1177 - acc: 0.9667 - val_loss: 2.0395 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.86667\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.2711 - acc: 0.800 - ETA: 4s - loss: 0.1360 - acc: 0.900 - ETA: 4s - loss: 0.1025 - acc: 0.933 - ETA: 4s - loss: 0.1289 - acc: 0.900 - ETA: 3s - loss: 0.1147 - acc: 0.920 - ETA: 3s - loss: 0.1056 - acc: 0.933 - ETA: 3s - loss: 0.0962 - acc: 0.942 - ETA: 3s - loss: 0.0842 - acc: 0.950 - ETA: 3s - loss: 0.0764 - acc: 0.955 - ETA: 3s - loss: 0.0878 - acc: 0.940 - ETA: 3s - loss: 0.0828 - acc: 0.945 - ETA: 2s - loss: 0.0766 - acc: 0.950 - ETA: 2s - loss: 0.0756 - acc: 0.953 - ETA: 2s - loss: 0.0737 - acc: 0.957 - ETA: 2s - loss: 0.0688 - acc: 0.960 - ETA: 2s - loss: 0.0646 - acc: 0.962 - ETA: 2s - loss: 0.0612 - acc: 0.964 - ETA: 1s - loss: 0.0578 - acc: 0.966 - ETA: 1s - loss: 0.0547 - acc: 0.968 - ETA: 1s - loss: 0.0572 - acc: 0.970 - ETA: 1s - loss: 0.0553 - acc: 0.971 - ETA: 1s - loss: 0.0625 - acc: 0.963 - ETA: 1s - loss: 0.0598 - acc: 0.965 - ETA: 0s - loss: 0.0575 - acc: 0.966 - ETA: 0s - loss: 0.0748 - acc: 0.960 - ETA: 0s - loss: 0.0759 - acc: 0.961 - ETA: 0s - loss: 0.0737 - acc: 0.963 - ETA: 0s - loss: 0.0713 - acc: 0.964 - ETA: 0s - loss: 0.0688 - acc: 0.965 - 5s 167ms/step - loss: 0.0797 - acc: 0.9533 - val_loss: 3.2036 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86667\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.1538e-04 - acc: 1.000 - ETA: 4s - loss: 0.0017 - acc: 1.0000    - ETA: 4s - loss: 0.0013 - acc: 1.000 - ETA: 4s - loss: 0.1290 - acc: 0.950 - ETA: 4s - loss: 0.1134 - acc: 0.960 - ETA: 3s - loss: 0.1330 - acc: 0.933 - ETA: 3s - loss: 0.1140 - acc: 0.942 - ETA: 3s - loss: 0.1021 - acc: 0.950 - ETA: 3s - loss: 0.1986 - acc: 0.911 - ETA: 3s - loss: 0.1797 - acc: 0.920 - ETA: 3s - loss: 0.1640 - acc: 0.927 - ETA: 2s - loss: 0.1504 - acc: 0.933 - ETA: 2s - loss: 0.1390 - acc: 0.938 - ETA: 2s - loss: 0.1293 - acc: 0.942 - ETA: 2s - loss: 0.1215 - acc: 0.946 - ETA: 2s - loss: 0.1390 - acc: 0.937 - ETA: 2s - loss: 0.1311 - acc: 0.941 - ETA: 1s - loss: 0.1245 - acc: 0.944 - ETA: 1s - loss: 0.1181 - acc: 0.947 - ETA: 1s - loss: 0.1134 - acc: 0.950 - ETA: 1s - loss: 0.1083 - acc: 0.952 - ETA: 1s - loss: 0.1044 - acc: 0.954 - ETA: 1s - loss: 0.1006 - acc: 0.956 - ETA: 0s - loss: 0.0976 - acc: 0.958 - ETA: 0s - loss: 0.0944 - acc: 0.960 - ETA: 0s - loss: 0.0911 - acc: 0.961 - ETA: 0s - loss: 0.0877 - acc: 0.963 - ETA: 0s - loss: 0.0875 - acc: 0.964 - ETA: 0s - loss: 0.0846 - acc: 0.965 - 5s 167ms/step - loss: 0.0893 - acc: 0.9600 - val_loss: 1.5117 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86667\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.1590 - acc: 1.000 - ETA: 4s - loss: 0.1430 - acc: 1.000 - ETA: 4s - loss: 0.1114 - acc: 1.000 - ETA: 4s - loss: 0.0866 - acc: 1.000 - ETA: 3s - loss: 0.2016 - acc: 0.960 - ETA: 3s - loss: 0.1696 - acc: 0.966 - ETA: 3s - loss: 0.1460 - acc: 0.971 - ETA: 3s - loss: 0.1281 - acc: 0.975 - ETA: 3s - loss: 0.1141 - acc: 0.977 - ETA: 3s - loss: 0.1039 - acc: 0.980 - ETA: 3s - loss: 0.1041 - acc: 0.981 - ETA: 2s - loss: 0.0965 - acc: 0.983 - ETA: 2s - loss: 0.0898 - acc: 0.984 - ETA: 2s - loss: 0.0835 - acc: 0.985 - ETA: 2s - loss: 0.0786 - acc: 0.986 - ETA: 2s - loss: 0.0745 - acc: 0.987 - ETA: 2s - loss: 0.0706 - acc: 0.988 - ETA: 1s - loss: 0.0724 - acc: 0.988 - ETA: 1s - loss: 0.0689 - acc: 0.989 - ETA: 1s - loss: 0.0656 - acc: 0.990 - ETA: 1s - loss: 0.0627 - acc: 0.990 - ETA: 1s - loss: 0.1003 - acc: 0.981 - ETA: 1s - loss: 0.0997 - acc: 0.982 - ETA: 0s - loss: 0.0968 - acc: 0.983 - ETA: 0s - loss: 0.0953 - acc: 0.984 - ETA: 0s - loss: 0.0931 - acc: 0.984 - ETA: 0s - loss: 0.1251 - acc: 0.977 - ETA: 0s - loss: 0.1208 - acc: 0.978 - ETA: 0s - loss: 0.1170 - acc: 0.979 - 5s 166ms/step - loss: 0.1612 - acc: 0.9667 - val_loss: 1.4305 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.86667\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0666 - acc: 1.000 - ETA: 4s - loss: 0.3394 - acc: 0.900 - ETA: 4s - loss: 0.3089 - acc: 0.866 - ETA: 4s - loss: 0.2892 - acc: 0.900 - ETA: 3s - loss: 0.2473 - acc: 0.920 - ETA: 3s - loss: 0.2073 - acc: 0.933 - ETA: 3s - loss: 0.1794 - acc: 0.942 - ETA: 3s - loss: 0.1581 - acc: 0.950 - ETA: 3s - loss: 0.1424 - acc: 0.955 - ETA: 3s - loss: 0.1322 - acc: 0.960 - ETA: 3s - loss: 0.1736 - acc: 0.945 - ETA: 2s - loss: 0.1595 - acc: 0.950 - ETA: 2s - loss: 0.1540 - acc: 0.953 - ETA: 2s - loss: 0.1494 - acc: 0.957 - ETA: 2s - loss: 0.1426 - acc: 0.960 - ETA: 2s - loss: 0.1338 - acc: 0.962 - ETA: 2s - loss: 0.1412 - acc: 0.952 - ETA: 1s - loss: 0.1349 - acc: 0.955 - ETA: 1s - loss: 0.1428 - acc: 0.947 - ETA: 1s - loss: 0.1360 - acc: 0.950 - ETA: 1s - loss: 0.1312 - acc: 0.952 - ETA: 1s - loss: 0.1253 - acc: 0.954 - ETA: 1s - loss: 0.1205 - acc: 0.956 - ETA: 0s - loss: 0.1155 - acc: 0.958 - ETA: 0s - loss: 0.1596 - acc: 0.952 - ETA: 0s - loss: 0.1535 - acc: 0.953 - ETA: 0s - loss: 0.1478 - acc: 0.955 - ETA: 0s - loss: 0.1546 - acc: 0.950 - ETA: 0s - loss: 0.1501 - acc: 0.951 - 5s 165ms/step - loss: 0.1490 - acc: 0.9533 - val_loss: 1.9350 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.86667\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 0.0113 - acc: 1.000 - ETA: 4s - loss: 0.0087 - acc: 1.000 - ETA: 4s - loss: 0.0073 - acc: 1.000 - ETA: 4s - loss: 0.0084 - acc: 1.000 - ETA: 3s - loss: 0.0107 - acc: 1.000 - ETA: 3s - loss: 0.0106 - acc: 1.000 - ETA: 3s - loss: 0.0092 - acc: 1.000 - ETA: 3s - loss: 0.0091 - acc: 1.000 - ETA: 3s - loss: 0.0640 - acc: 0.955 - ETA: 3s - loss: 0.1579 - acc: 0.920 - ETA: 3s - loss: 0.1443 - acc: 0.927 - ETA: 2s - loss: 0.1325 - acc: 0.933 - ETA: 2s - loss: 0.1225 - acc: 0.938 - ETA: 2s - loss: 0.1288 - acc: 0.928 - ETA: 2s - loss: 0.1262 - acc: 0.933 - ETA: 2s - loss: 0.1220 - acc: 0.937 - ETA: 2s - loss: 0.1158 - acc: 0.941 - ETA: 1s - loss: 0.1119 - acc: 0.944 - ETA: 1s - loss: 0.1161 - acc: 0.936 - ETA: 1s - loss: 0.1183 - acc: 0.930 - ETA: 1s - loss: 0.1139 - acc: 0.933 - ETA: 1s - loss: 0.1142 - acc: 0.936 - ETA: 1s - loss: 0.1106 - acc: 0.939 - ETA: 0s - loss: 0.1065 - acc: 0.941 - ETA: 0s - loss: 0.1089 - acc: 0.944 - ETA: 0s - loss: 0.1061 - acc: 0.946 - ETA: 0s - loss: 0.1028 - acc: 0.948 - ETA: 0s - loss: 0.0997 - acc: 0.950 - ETA: 0s - loss: 0.1008 - acc: 0.951 - 5s 166ms/step - loss: 0.0988 - acc: 0.9533 - val_loss: 2.8390 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.86667\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.5224 - acc: 0.800 - ETA: 4s - loss: 0.2620 - acc: 0.900 - ETA: 4s - loss: 0.1755 - acc: 0.933 - ETA: 4s - loss: 0.4512 - acc: 0.800 - ETA: 3s - loss: 0.3654 - acc: 0.840 - ETA: 3s - loss: 0.4171 - acc: 0.833 - ETA: 3s - loss: 0.3589 - acc: 0.857 - ETA: 3s - loss: 0.3215 - acc: 0.875 - ETA: 3s - loss: 0.2885 - acc: 0.888 - ETA: 3s - loss: 0.3303 - acc: 0.880 - ETA: 3s - loss: 0.3042 - acc: 0.890 - ETA: 2s - loss: 0.3023 - acc: 0.883 - ETA: 2s - loss: 0.2816 - acc: 0.892 - ETA: 2s - loss: 0.2737 - acc: 0.900 - ETA: 2s - loss: 0.2773 - acc: 0.880 - ETA: 2s - loss: 0.2679 - acc: 0.887 - ETA: 2s - loss: 0.2525 - acc: 0.894 - ETA: 1s - loss: 0.2394 - acc: 0.900 - ETA: 1s - loss: 0.2271 - acc: 0.905 - ETA: 1s - loss: 0.2178 - acc: 0.910 - ETA: 1s - loss: 0.2117 - acc: 0.914 - ETA: 1s - loss: 0.2136 - acc: 0.909 - ETA: 1s - loss: 0.2087 - acc: 0.913 - ETA: 0s - loss: 0.2030 - acc: 0.916 - ETA: 0s - loss: 0.1950 - acc: 0.920 - ETA: 0s - loss: 0.1887 - acc: 0.923 - ETA: 0s - loss: 0.1932 - acc: 0.918 - ETA: 0s - loss: 0.1889 - acc: 0.921 - ETA: 0s - loss: 0.1835 - acc: 0.924 - 5s 166ms/step - loss: 0.1776 - acc: 0.9267 - val_loss: 1.1639 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.86667\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0095 - acc: 1.000 - ETA: 4s - loss: 0.0050 - acc: 1.000 - ETA: 4s - loss: 0.0263 - acc: 1.000 - ETA: 4s - loss: 0.0225 - acc: 1.000 - ETA: 3s - loss: 0.0307 - acc: 1.000 - ETA: 3s - loss: 0.0293 - acc: 1.000 - ETA: 3s - loss: 0.0257 - acc: 1.000 - ETA: 3s - loss: 0.2089 - acc: 0.975 - ETA: 3s - loss: 0.1899 - acc: 0.977 - ETA: 3s - loss: 0.1710 - acc: 0.980 - ETA: 3s - loss: 0.1554 - acc: 0.981 - ETA: 2s - loss: 0.1770 - acc: 0.966 - ETA: 2s - loss: 0.1989 - acc: 0.938 - ETA: 2s - loss: 0.1849 - acc: 0.942 - ETA: 2s - loss: 0.1732 - acc: 0.946 - ETA: 2s - loss: 0.1693 - acc: 0.950 - ETA: 2s - loss: 0.1606 - acc: 0.952 - ETA: 1s - loss: 0.1717 - acc: 0.944 - ETA: 1s - loss: 0.1735 - acc: 0.936 - ETA: 1s - loss: 0.1649 - acc: 0.940 - ETA: 1s - loss: 0.1579 - acc: 0.942 - ETA: 1s - loss: 0.1509 - acc: 0.945 - ETA: 1s - loss: 0.1444 - acc: 0.947 - ETA: 0s - loss: 0.1397 - acc: 0.950 - ETA: 0s - loss: 0.1384 - acc: 0.952 - ETA: 0s - loss: 0.1332 - acc: 0.953 - ETA: 0s - loss: 0.1297 - acc: 0.955 - ETA: 0s - loss: 0.1261 - acc: 0.957 - ETA: 0s - loss: 0.1220 - acc: 0.958 - 5s 165ms/step - loss: 0.1194 - acc: 0.9600 - val_loss: 1.1425 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.86667\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.5424 - acc: 0.800 - ETA: 4s - loss: 0.3000 - acc: 0.900 - ETA: 4s - loss: 0.2026 - acc: 0.933 - ETA: 4s - loss: 0.1546 - acc: 0.950 - ETA: 3s - loss: 0.1243 - acc: 0.960 - ETA: 3s - loss: 0.1071 - acc: 0.966 - ETA: 3s - loss: 0.0943 - acc: 0.971 - ETA: 3s - loss: 0.0918 - acc: 0.975 - ETA: 3s - loss: 0.0819 - acc: 0.977 - ETA: 3s - loss: 0.0793 - acc: 0.980 - ETA: 2s - loss: 0.0723 - acc: 0.981 - ETA: 2s - loss: 0.0680 - acc: 0.983 - ETA: 2s - loss: 0.0631 - acc: 0.984 - ETA: 2s - loss: 0.0725 - acc: 0.971 - ETA: 2s - loss: 0.0700 - acc: 0.973 - ETA: 2s - loss: 0.0660 - acc: 0.975 - ETA: 2s - loss: 0.0683 - acc: 0.976 - ETA: 1s - loss: 0.0651 - acc: 0.977 - ETA: 1s - loss: 0.0632 - acc: 0.978 - ETA: 1s - loss: 0.0608 - acc: 0.980 - ETA: 1s - loss: 0.0617 - acc: 0.981 - ETA: 1s - loss: 0.0614 - acc: 0.981 - ETA: 1s - loss: 0.0588 - acc: 0.982 - ETA: 0s - loss: 0.0587 - acc: 0.983 - ETA: 0s - loss: 0.0565 - acc: 0.984 - ETA: 0s - loss: 0.0544 - acc: 0.984 - ETA: 0s - loss: 0.0592 - acc: 0.977 - ETA: 0s - loss: 0.0571 - acc: 0.978 - ETA: 0s - loss: 0.0568 - acc: 0.979 - 5s 166ms/step - loss: 0.0549 - acc: 0.9800 - val_loss: 1.0067 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.86667\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0013 - acc: 1.000 - ETA: 4s - loss: 0.0027 - acc: 1.000 - ETA: 4s - loss: 0.0147 - acc: 1.000 - ETA: 4s - loss: 0.0125 - acc: 1.000 - ETA: 3s - loss: 0.0115 - acc: 1.000 - ETA: 3s - loss: 0.0101 - acc: 1.000 - ETA: 3s - loss: 0.0091 - acc: 1.000 - ETA: 3s - loss: 0.0080 - acc: 1.000 - ETA: 3s - loss: 0.0131 - acc: 1.000 - ETA: 3s - loss: 0.0119 - acc: 1.000 - ETA: 3s - loss: 0.0109 - acc: 1.000 - ETA: 2s - loss: 0.0106 - acc: 1.000 - ETA: 2s - loss: 0.0146 - acc: 1.000 - ETA: 2s - loss: 0.0140 - acc: 1.000 - ETA: 2s - loss: 0.0131 - acc: 1.000 - ETA: 2s - loss: 0.0250 - acc: 0.987 - ETA: 2s - loss: 0.0235 - acc: 0.988 - ETA: 1s - loss: 0.0223 - acc: 0.988 - ETA: 1s - loss: 0.0212 - acc: 0.989 - ETA: 1s - loss: 0.0201 - acc: 0.990 - ETA: 1s - loss: 0.0197 - acc: 0.990 - ETA: 1s - loss: 0.0188 - acc: 0.990 - ETA: 1s - loss: 0.0184 - acc: 0.991 - ETA: 0s - loss: 0.0177 - acc: 0.991 - ETA: 0s - loss: 0.0171 - acc: 0.992 - ETA: 0s - loss: 0.0165 - acc: 0.992 - ETA: 0s - loss: 0.0167 - acc: 0.992 - ETA: 0s - loss: 0.0161 - acc: 0.992 - ETA: 0s - loss: 0.0434 - acc: 0.986 - 5s 166ms/step - loss: 0.0420 - acc: 0.9867 - val_loss: 1.2670 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.86667\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      "30/30 [==============================] - ETA: 4s - loss: 6.5279e-04 - acc: 1.000 - ETA: 4s - loss: 5.0402e-04 - acc: 1.000 - ETA: 4s - loss: 0.0123 - acc: 1.0000    - ETA: 4s - loss: 0.0102 - acc: 1.000 - ETA: 3s - loss: 0.0082 - acc: 1.000 - ETA: 3s - loss: 0.0094 - acc: 1.000 - ETA: 3s - loss: 0.0081 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0066 - acc: 1.000 - ETA: 3s - loss: 0.0087 - acc: 1.000 - ETA: 3s - loss: 0.0079 - acc: 1.000 - ETA: 2s - loss: 0.0073 - acc: 1.000 - ETA: 2s - loss: 0.0067 - acc: 1.000 - ETA: 2s - loss: 0.0063 - acc: 1.000 - ETA: 2s - loss: 0.0070 - acc: 1.000 - ETA: 2s - loss: 0.0065 - acc: 1.000 - ETA: 2s - loss: 0.0062 - acc: 1.000 - ETA: 1s - loss: 0.0059 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 1.000 - ETA: 1s - loss: 0.0062 - acc: 1.000 - ETA: 1s - loss: 0.0073 - acc: 1.000 - ETA: 1s - loss: 0.0096 - acc: 1.000 - ETA: 1s - loss: 0.0092 - acc: 1.000 - ETA: 0s - loss: 0.0205 - acc: 0.991 - ETA: 0s - loss: 0.0197 - acc: 0.992 - ETA: 0s - loss: 0.0190 - acc: 0.992 - ETA: 0s - loss: 0.0208 - acc: 0.992 - ETA: 0s - loss: 0.0201 - acc: 0.992 - ETA: 0s - loss: 0.0194 - acc: 0.993 - 5s 166ms/step - loss: 0.0188 - acc: 0.9933 - val_loss: 1.0761 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.86667\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 3.7210e-04 - acc: 1.000 - ETA: 4s - loss: 2.2372e-04 - acc: 1.000 - ETA: 4s - loss: 1.6356e-04 - acc: 1.000 - ETA: 4s - loss: 6.3807e-04 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.0000    - ETA: 3s - loss: 0.0056 - acc: 1.000 - ETA: 3s - loss: 0.0476 - acc: 0.971 - ETA: 3s - loss: 0.0417 - acc: 0.975 - ETA: 3s - loss: 0.0371 - acc: 0.977 - ETA: 3s - loss: 0.0334 - acc: 0.980 - ETA: 3s - loss: 0.0304 - acc: 0.981 - ETA: 2s - loss: 0.0279 - acc: 0.983 - ETA: 2s - loss: 0.0259 - acc: 0.984 - ETA: 2s - loss: 0.0241 - acc: 0.985 - ETA: 2s - loss: 0.0225 - acc: 0.986 - ETA: 2s - loss: 0.0212 - acc: 0.987 - ETA: 2s - loss: 0.0200 - acc: 0.988 - ETA: 1s - loss: 0.0192 - acc: 0.988 - ETA: 1s - loss: 0.0182 - acc: 0.989 - ETA: 1s - loss: 0.0190 - acc: 0.990 - ETA: 1s - loss: 0.0181 - acc: 0.990 - ETA: 1s - loss: 0.0173 - acc: 0.990 - ETA: 1s - loss: 0.0166 - acc: 0.991 - ETA: 0s - loss: 0.0160 - acc: 0.991 - ETA: 0s - loss: 0.0153 - acc: 0.992 - ETA: 0s - loss: 0.0147 - acc: 0.992 - ETA: 0s - loss: 0.0470 - acc: 0.977 - ETA: 0s - loss: 0.0454 - acc: 0.978 - ETA: 0s - loss: 0.0438 - acc: 0.979 - 5s 166ms/step - loss: 0.0424 - acc: 0.9800 - val_loss: 1.7813 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.86667\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0029 - acc: 1.000 - ETA: 4s - loss: 0.0023 - acc: 1.000 - ETA: 4s - loss: 0.0027 - acc: 1.000 - ETA: 4s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0053 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0044 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 2s - loss: 0.0037 - acc: 1.000 - ETA: 2s - loss: 0.0036 - acc: 1.000 - ETA: 2s - loss: 0.0044 - acc: 1.000 - ETA: 2s - loss: 0.0079 - acc: 1.000 - ETA: 2s - loss: 0.0075 - acc: 1.000 - ETA: 2s - loss: 0.0071 - acc: 1.000 - ETA: 1s - loss: 0.0067 - acc: 1.000 - ETA: 1s - loss: 0.0069 - acc: 1.000 - ETA: 1s - loss: 0.0065 - acc: 1.000 - ETA: 1s - loss: 0.0062 - acc: 1.000 - ETA: 1s - loss: 0.0061 - acc: 1.000 - ETA: 1s - loss: 0.0074 - acc: 1.000 - ETA: 0s - loss: 0.0071 - acc: 1.000 - ETA: 0s - loss: 0.0068 - acc: 1.000 - ETA: 0s - loss: 0.0066 - acc: 1.000 - ETA: 0s - loss: 0.0069 - acc: 1.000 - ETA: 0s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0066 - acc: 1.000 - 5s 166ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 1.4778 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86667\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0058 - acc: 1.000 - ETA: 4s - loss: 0.0033 - acc: 1.000 - ETA: 4s - loss: 0.0031 - acc: 1.000 - ETA: 4s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0026 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 1.000 - ETA: 2s - loss: 0.0022 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0033 - acc: 1.000 - ETA: 0s - loss: 0.0032 - acc: 1.000 - ETA: 0s - loss: 0.0043 - acc: 1.000 - 5s 166ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 1.2261 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86667\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.1661e-04 - acc: 1.000 - ETA: 4s - loss: 0.0361 - acc: 1.0000    - ETA: 4s - loss: 0.0245 - acc: 1.000 - ETA: 4s - loss: 0.0186 - acc: 1.000 - ETA: 3s - loss: 0.0152 - acc: 1.000 - ETA: 3s - loss: 0.0129 - acc: 1.000 - ETA: 3s - loss: 0.0115 - acc: 1.000 - ETA: 3s - loss: 0.0288 - acc: 0.975 - ETA: 3s - loss: 0.0256 - acc: 0.977 - ETA: 3s - loss: 0.0232 - acc: 0.980 - ETA: 2s - loss: 0.0212 - acc: 0.981 - ETA: 2s - loss: 0.0195 - acc: 0.983 - ETA: 2s - loss: 0.0180 - acc: 0.984 - ETA: 2s - loss: 0.0169 - acc: 0.985 - ETA: 2s - loss: 0.0167 - acc: 0.986 - ETA: 2s - loss: 0.0158 - acc: 0.987 - ETA: 2s - loss: 0.0149 - acc: 0.988 - ETA: 1s - loss: 0.0141 - acc: 0.988 - ETA: 1s - loss: 0.0134 - acc: 0.989 - ETA: 1s - loss: 0.0128 - acc: 0.990 - ETA: 1s - loss: 0.0122 - acc: 0.990 - ETA: 1s - loss: 0.0120 - acc: 0.990 - ETA: 1s - loss: 0.0116 - acc: 0.991 - ETA: 0s - loss: 0.0112 - acc: 0.991 - ETA: 0s - loss: 0.0108 - acc: 0.992 - ETA: 0s - loss: 0.0104 - acc: 0.992 - ETA: 0s - loss: 0.0101 - acc: 0.992 - ETA: 0s - loss: 0.0100 - acc: 0.992 - ETA: 0s - loss: 0.0097 - acc: 0.993 - 5s 165ms/step - loss: 0.0094 - acc: 0.9933 - val_loss: 1.1226 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86667\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 9.9720e-04 - acc: 1.000 - ETA: 4s - loss: 0.2710 - acc: 0.9000    - ETA: 4s - loss: 0.1807 - acc: 0.933 - ETA: 4s - loss: 0.1357 - acc: 0.950 - ETA: 3s - loss: 0.1086 - acc: 0.960 - ETA: 3s - loss: 0.0905 - acc: 0.966 - ETA: 3s - loss: 0.0778 - acc: 0.971 - ETA: 3s - loss: 0.0696 - acc: 0.975 - ETA: 3s - loss: 0.0627 - acc: 0.977 - ETA: 3s - loss: 0.0566 - acc: 0.980 - ETA: 3s - loss: 0.0525 - acc: 0.981 - ETA: 2s - loss: 0.0482 - acc: 0.983 - ETA: 2s - loss: 0.0445 - acc: 0.984 - ETA: 2s - loss: 0.0415 - acc: 0.985 - ETA: 2s - loss: 0.0388 - acc: 0.986 - ETA: 2s - loss: 0.0364 - acc: 0.987 - ETA: 2s - loss: 0.0343 - acc: 0.988 - ETA: 1s - loss: 0.0325 - acc: 0.988 - ETA: 1s - loss: 0.0309 - acc: 0.989 - ETA: 1s - loss: 0.0294 - acc: 0.990 - ETA: 1s - loss: 0.0280 - acc: 0.990 - ETA: 1s - loss: 0.0268 - acc: 0.990 - ETA: 1s - loss: 0.0260 - acc: 0.991 - ETA: 0s - loss: 0.0250 - acc: 0.991 - ETA: 0s - loss: 0.0240 - acc: 0.992 - ETA: 0s - loss: 0.0245 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0228 - acc: 0.992 - ETA: 0s - loss: 0.0222 - acc: 0.993 - 5s 166ms/step - loss: 0.0215 - acc: 0.9933 - val_loss: 1.0162 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.86667\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 6.6066e-04 - acc: 1.000 - ETA: 4s - loss: 0.0022 - acc: 1.0000    - ETA: 4s - loss: 0.0019 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 0.0011 - acc: 1.000 - ETA: 2s - loss: 0.0011 - acc: 1.000 - ETA: 2s - loss: 0.0010 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0021 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0233 - acc: 0.992 - ETA: 0s - loss: 0.0228 - acc: 0.992 - ETA: 0s - loss: 0.0220 - acc: 0.992 - ETA: 0s - loss: 0.0212 - acc: 0.992 - ETA: 0s - loss: 0.0206 - acc: 0.993 - 5s 165ms/step - loss: 0.0199 - acc: 0.9933 - val_loss: 0.9960 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.86667\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 2.4536e-04 - acc: 1.000 - ETA: 4s - loss: 0.0051 - acc: 1.0000    - ETA: 4s - loss: 0.0041 - acc: 1.000 - ETA: 4s - loss: 0.0037 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0031 - acc: 1.000 - ETA: 2s - loss: 0.0055 - acc: 1.000 - ETA: 2s - loss: 0.0149 - acc: 0.987 - ETA: 2s - loss: 0.0141 - acc: 0.988 - ETA: 1s - loss: 0.0135 - acc: 0.988 - ETA: 1s - loss: 0.0128 - acc: 0.989 - ETA: 1s - loss: 0.0122 - acc: 0.990 - ETA: 1s - loss: 0.0180 - acc: 0.990 - ETA: 1s - loss: 0.0173 - acc: 0.990 - ETA: 1s - loss: 0.0166 - acc: 0.991 - ETA: 0s - loss: 0.0248 - acc: 0.983 - ETA: 0s - loss: 0.0238 - acc: 0.984 - ETA: 0s - loss: 0.0230 - acc: 0.984 - ETA: 0s - loss: 0.0221 - acc: 0.985 - ETA: 0s - loss: 0.0213 - acc: 0.985 - ETA: 0s - loss: 0.0244 - acc: 0.986 - 5s 166ms/step - loss: 0.0236 - acc: 0.9867 - val_loss: 0.9886 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.86667\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0151 - acc: 1.000 - ETA: 4s - loss: 0.0083 - acc: 1.000 - ETA: 4s - loss: 0.0056 - acc: 1.000 - ETA: 4s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0036 - acc: 1.000 - ETA: 3s - loss: 0.0034 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0037 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 1s - loss: 0.0033 - acc: 1.000 - ETA: 1s - loss: 0.0031 - acc: 1.000 - ETA: 1s - loss: 0.0030 - acc: 1.000 - ETA: 1s - loss: 0.0030 - acc: 1.000 - ETA: 1s - loss: 0.0028 - acc: 1.000 - ETA: 1s - loss: 0.0244 - acc: 0.991 - ETA: 0s - loss: 0.0234 - acc: 0.991 - ETA: 0s - loss: 0.0225 - acc: 0.992 - ETA: 0s - loss: 0.0217 - acc: 0.992 - ETA: 0s - loss: 0.0209 - acc: 0.992 - ETA: 0s - loss: 0.0202 - acc: 0.992 - ETA: 0s - loss: 0.0196 - acc: 0.993 - 5s 166ms/step - loss: 0.0204 - acc: 0.9933 - val_loss: 0.9657 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.86667\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0231 - acc: 1.000 - ETA: 4s - loss: 0.0120 - acc: 1.000 - ETA: 4s - loss: 0.0087 - acc: 1.000 - ETA: 4s - loss: 0.0070 - acc: 1.000 - ETA: 3s - loss: 0.0057 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0044 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0037 - acc: 1.000 - ETA: 3s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0038 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 1.000 - ETA: 1s - loss: 0.0359 - acc: 0.989 - ETA: 1s - loss: 0.0344 - acc: 0.990 - ETA: 1s - loss: 0.0328 - acc: 0.990 - ETA: 1s - loss: 0.0314 - acc: 0.990 - ETA: 1s - loss: 0.0300 - acc: 0.991 - ETA: 0s - loss: 0.0288 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.992 - ETA: 0s - loss: 0.0240 - acc: 0.993 - 5s 165ms/step - loss: 0.0232 - acc: 0.9933 - val_loss: 0.9117 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.86667\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.5800e-04 - acc: 1.000 - ETA: 4s - loss: 3.7871e-04 - acc: 1.000 - ETA: 4s - loss: 3.1693e-04 - acc: 1.000 - ETA: 4s - loss: 2.7756e-04 - acc: 1.000 - ETA: 3s - loss: 0.0113 - acc: 1.0000    - ETA: 3s - loss: 0.0094 - acc: 1.000 - ETA: 3s - loss: 0.0085 - acc: 1.000 - ETA: 3s - loss: 0.0075 - acc: 1.000 - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 3s - loss: 0.0061 - acc: 1.000 - ETA: 2s - loss: 0.0056 - acc: 1.000 - ETA: 2s - loss: 0.0052 - acc: 1.000 - ETA: 2s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0048 - acc: 1.000 - ETA: 2s - loss: 0.0046 - acc: 1.000 - ETA: 2s - loss: 0.0045 - acc: 1.000 - ETA: 2s - loss: 0.0044 - acc: 1.000 - ETA: 1s - loss: 0.0042 - acc: 1.000 - ETA: 1s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 0.0039 - acc: 1.000 - ETA: 1s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0034 - acc: 1.000 - ETA: 0s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0035 - acc: 1.000 - ETA: 0s - loss: 0.0034 - acc: 1.000 - ETA: 0s - loss: 0.0033 - acc: 1.000 - 5s 165ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.9161 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.86667\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.7909e-04 - acc: 1.000 - ETA: 4s - loss: 2.3190e-04 - acc: 1.000 - ETA: 4s - loss: 0.0011 - acc: 1.0000    - ETA: 4s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0077 - acc: 1.000 - ETA: 3s - loss: 0.0064 - acc: 1.000 - ETA: 3s - loss: 0.0056 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0044 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0236 - acc: 0.981 - ETA: 2s - loss: 0.0217 - acc: 0.983 - ETA: 2s - loss: 0.0354 - acc: 0.969 - ETA: 2s - loss: 0.0330 - acc: 0.971 - ETA: 2s - loss: 0.0308 - acc: 0.973 - ETA: 2s - loss: 0.0290 - acc: 0.975 - ETA: 2s - loss: 0.0273 - acc: 0.976 - ETA: 1s - loss: 0.0364 - acc: 0.966 - ETA: 1s - loss: 0.0363 - acc: 0.968 - ETA: 1s - loss: 0.0345 - acc: 0.970 - ETA: 1s - loss: 0.0335 - acc: 0.971 - ETA: 1s - loss: 0.0321 - acc: 0.972 - ETA: 1s - loss: 0.0307 - acc: 0.973 - ETA: 0s - loss: 0.0295 - acc: 0.975 - ETA: 0s - loss: 0.0283 - acc: 0.976 - ETA: 0s - loss: 0.0273 - acc: 0.976 - ETA: 0s - loss: 0.0271 - acc: 0.977 - ETA: 0s - loss: 0.0262 - acc: 0.978 - ETA: 0s - loss: 0.0253 - acc: 0.979 - 5s 167ms/step - loss: 0.0248 - acc: 0.9800 - val_loss: 0.9108 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.86667\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 3.4518e-04 - acc: 1.000 - ETA: 4s - loss: 1.9139e-04 - acc: 1.000 - ETA: 4s - loss: 7.2667e-04 - acc: 1.000 - ETA: 4s - loss: 0.0018 - acc: 1.0000    - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0019 - acc: 1.000 - ETA: 2s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 1s - loss: 0.0028 - acc: 1.000 - ETA: 1s - loss: 0.0244 - acc: 0.989 - ETA: 1s - loss: 0.0232 - acc: 0.990 - ETA: 1s - loss: 0.0223 - acc: 0.990 - ETA: 1s - loss: 0.0213 - acc: 0.990 - ETA: 1s - loss: 0.0204 - acc: 0.991 - ETA: 0s - loss: 0.0195 - acc: 0.991 - ETA: 0s - loss: 0.0188 - acc: 0.992 - ETA: 0s - loss: 0.0181 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.985 - ETA: 0s - loss: 0.0241 - acc: 0.985 - ETA: 0s - loss: 0.0233 - acc: 0.986 - 5s 165ms/step - loss: 0.0225 - acc: 0.9867 - val_loss: 0.9561 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.86667\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 7.8453e-04 - acc: 1.000 - ETA: 4s - loss: 0.0010 - acc: 1.0000    - ETA: 4s - loss: 0.0023 - acc: 1.000 - ETA: 4s - loss: 0.0024 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0054 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0046 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0039 - acc: 1.000 - ETA: 2s - loss: 0.0041 - acc: 1.000 - ETA: 2s - loss: 0.0038 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0041 - acc: 1.000 - ETA: 2s - loss: 0.0038 - acc: 1.000 - ETA: 1s - loss: 0.0037 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0036 - acc: 1.000 - ETA: 1s - loss: 0.0035 - acc: 1.000 - ETA: 1s - loss: 0.0033 - acc: 1.000 - ETA: 1s - loss: 0.0032 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0030 - acc: 1.000 - ETA: 0s - loss: 0.0029 - acc: 1.000 - ETA: 0s - loss: 0.0031 - acc: 1.000 - ETA: 0s - loss: 0.0029 - acc: 1.000 - ETA: 0s - loss: 0.0029 - acc: 1.000 - 5s 166ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9661 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.86667\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 5.8892e-04 - acc: 1.000 - ETA: 4s - loss: 9.1247e-04 - acc: 1.000 - ETA: 4s - loss: 0.0011 - acc: 1.0000    - ETA: 3s - loss: 0.0102 - acc: 1.000 - ETA: 3s - loss: 0.0086 - acc: 1.000 - ETA: 3s - loss: 0.0075 - acc: 1.000 - ETA: 3s - loss: 0.0066 - acc: 1.000 - ETA: 3s - loss: 0.0067 - acc: 1.000 - ETA: 3s - loss: 0.0061 - acc: 1.000 - ETA: 3s - loss: 0.0068 - acc: 1.000 - ETA: 2s - loss: 0.0083 - acc: 1.000 - ETA: 2s - loss: 0.0077 - acc: 1.000 - ETA: 2s - loss: 0.0072 - acc: 1.000 - ETA: 2s - loss: 0.0067 - acc: 1.000 - ETA: 2s - loss: 0.0063 - acc: 1.000 - ETA: 2s - loss: 0.0061 - acc: 1.000 - ETA: 1s - loss: 0.0058 - acc: 1.000 - ETA: 1s - loss: 0.0055 - acc: 1.000 - ETA: 1s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0051 - acc: 1.000 - ETA: 1s - loss: 0.0049 - acc: 1.000 - ETA: 0s - loss: 0.0047 - acc: 1.000 - ETA: 0s - loss: 0.0050 - acc: 1.000 - ETA: 0s - loss: 0.0049 - acc: 1.000 - ETA: 0s - loss: 0.0048 - acc: 1.000 - ETA: 0s - loss: 0.0047 - acc: 1.000 - ETA: 0s - loss: 0.0046 - acc: 1.000 - 5s 166ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.9825 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.86667\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 2.3591e-04 - acc: 1.000 - ETA: 4s - loss: 6.9118e-04 - acc: 1.000 - ETA: 4s - loss: 0.0021 - acc: 1.0000    - ETA: 4s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0023 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 1.000 - ETA: 2s - loss: 0.0023 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0065 - acc: 1.000 - ETA: 1s - loss: 0.0062 - acc: 1.000 - ETA: 1s - loss: 0.0059 - acc: 1.000 - ETA: 1s - loss: 0.0057 - acc: 1.000 - ETA: 1s - loss: 0.0056 - acc: 1.000 - ETA: 0s - loss: 0.0054 - acc: 1.000 - ETA: 0s - loss: 0.0053 - acc: 1.000 - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 0s - loss: 0.0051 - acc: 1.000 - ETA: 0s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0065 - acc: 1.000 - 5s 166ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.9783 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.86667\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 6.4734e-04 - acc: 1.000 - ETA: 4s - loss: 5.7095e-04 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.0000    - ETA: 4s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0013 - acc: 1.000 - ETA: 3s - loss: 0.0012 - acc: 1.000 - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0014 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0019 - acc: 1.000 - ETA: 2s - loss: 0.0018 - acc: 1.000 - ETA: 2s - loss: 0.0017 - acc: 1.000 - ETA: 2s - loss: 0.0021 - acc: 1.000 - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 1s - loss: 0.0019 - acc: 1.000 - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 1s - loss: 0.0018 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0017 - acc: 1.000 - ETA: 0s - loss: 0.0025 - acc: 1.000 - ETA: 0s - loss: 0.0024 - acc: 1.000 - 5s 166ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.9827 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.86667\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.6063e-04 - acc: 1.000 - ETA: 4s - loss: 8.3472e-04 - acc: 1.000 - ETA: 4s - loss: 9.1727e-04 - acc: 1.000 - ETA: 4s - loss: 9.7085e-04 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.0000    - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 3s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0026 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0026 - acc: 1.000 - ETA: 2s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0026 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0024 - acc: 1.000 - ETA: 0s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - 5s 165ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9808 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.86667\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 0.0051 - acc: 1.000 - ETA: 4s - loss: 0.0050 - acc: 1.000 - ETA: 4s - loss: 0.0041 - acc: 1.000 - ETA: 4s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0018 - acc: 1.000 - ETA: 2s - loss: 0.0016 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 2s - loss: 0.0015 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0014 - acc: 1.000 - ETA: 2s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0013 - acc: 1.000 - ETA: 1s - loss: 0.0015 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 1s - loss: 0.0014 - acc: 1.000 - ETA: 0s - loss: 0.0013 - acc: 1.000 - ETA: 0s - loss: 0.0015 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - 5s 165ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.9859 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.86667\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 4s - loss: 2.0481e-04 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.0000    - ETA: 4s - loss: 0.0024 - acc: 1.000 - ETA: 4s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0028 - acc: 1.000 - ETA: 2s - loss: 0.0026 - acc: 1.000 - ETA: 2s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0025 - acc: 1.000 - ETA: 2s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 1s - loss: 0.0022 - acc: 1.000 - ETA: 1s - loss: 0.0022 - acc: 1.000 - ETA: 1s - loss: 0.0021 - acc: 1.000 - ETA: 1s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0065 - acc: 1.000 - ETA: 0s - loss: 0.0065 - acc: 1.000 - ETA: 0s - loss: 0.0063 - acc: 1.000 - ETA: 0s - loss: 0.0061 - acc: 1.000 - ETA: 0s - loss: 0.0059 - acc: 1.000 - 5s 166ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.9768 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.86667\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      "30/30 [==============================] - ETA: 4s - loss: 1.4148e-04 - acc: 1.000 - ETA: 4s - loss: 0.0021 - acc: 1.0000    - ETA: 4s - loss: 0.0027 - acc: 1.000 - ETA: 4s - loss: 0.0022 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0018 - acc: 1.000 - ETA: 3s - loss: 0.0017 - acc: 1.000 - ETA: 3s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0031 - acc: 1.000 - ETA: 2s - loss: 0.0029 - acc: 1.000 - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 2s - loss: 0.0028 - acc: 1.000 - ETA: 2s - loss: 0.0028 - acc: 1.000 - ETA: 2s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0027 - acc: 1.000 - ETA: 1s - loss: 0.0026 - acc: 1.000 - ETA: 1s - loss: 0.0025 - acc: 1.000 - ETA: 1s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0023 - acc: 1.000 - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0021 - acc: 1.000 - ETA: 0s - loss: 0.0020 - acc: 1.000 - ETA: 0s - loss: 0.0024 - acc: 1.000 - ETA: 0s - loss: 0.0024 - acc: 1.000 - 5s 166ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.9750 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.86667\n"
     ]
    }
   ],
   "source": [
    "train_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juicebox  :  99.98624324798584\n",
      "chips  :  0.013753905659541488\n"
     ]
    }
   ],
   "source": [
    "run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis Environment",
   "language": "python",
   "name": "thesis2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
